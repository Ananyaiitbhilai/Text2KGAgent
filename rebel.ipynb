{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline and tokenizer once\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "def extract_text_triplets(input_text):\n",
    "    \"\"\"\n",
    "    Extracts triplets from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The text from which to extract triplets.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each representing a triplet with 'head', 'type', and 'tail'.\n",
    "    \"\"\"\n",
    "    # Use the tokenizer manually since we need special tokens\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode([\n",
    "        triplet_extractor(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]\n",
    "    ])\n",
    "\n",
    "    # Function to parse the generated text and extract the triplets\n",
    "    def extract_triplets(text):\n",
    "        triplets = []\n",
    "        relation, subject, object_ = '', '', ''\n",
    "        text = text.strip()\n",
    "        current = 'x'\n",
    "        for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "            if token == \"<triplet>\":\n",
    "                current = 't'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                    relation = ''\n",
    "                subject = ''\n",
    "            elif token == \"<subj>\":\n",
    "                current = 's'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                object_ = ''\n",
    "            elif token == \"<obj>\":\n",
    "                current = 'o'\n",
    "                relation = ''\n",
    "            else:\n",
    "                if current == 't':\n",
    "                    subject += ' ' + token\n",
    "                elif current == 's':\n",
    "                    object_ += ' ' + token\n",
    "                elif current == 'o':\n",
    "                    relation += ' ' + token\n",
    "        if subject != '' and relation != '' and object_ != '':\n",
    "            triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "        return triplets\n",
    "\n",
    "    extracted_triplets = extract_triplets(extracted_text[0])\n",
    "    return extracted_triplets\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\"\n",
    "extracted_triplets = extract_text_triplets(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'head': 'Punta Cana', 'type': 'located in the administrative territorial entity', 'tail': 'La Altagracia Province'}, {'head': 'Punta Cana', 'type': 'country', 'tail': 'Dominican Republic'}, {'head': 'Higuey', 'type': 'located in the administrative territorial entity', 'tail': 'La Altagracia Province'}, {'head': 'Higuey', 'type': 'country', 'tail': 'Dominican Republic'}, {'head': 'La Altagracia Province', 'type': 'country', 'tail': 'Dominican Republic'}, {'head': 'Dominican Republic', 'type': 'contains administrative territorial entity', 'tail': 'La Altagracia Province'}]\n"
     ]
    }
   ],
   "source": [
    "print(extracted_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"obama is president of usa.\"\n",
    "extracted_triplets1 = extract_text_triplets(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': 'Obama', 'type': 'position held', 'tail': 'president of usa'},\n",
       " {'head': 'president of usa', 'type': 'officeholder', 'tail': 'Obama'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_triplets1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/ccc66ybs0m59z0yyhly4_jp80000gn/T/ipykernel_10502/661250872.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3877.58 MiB, ( 3877.64 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3877.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 4135.45 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, ( 4299.47 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf\",  \n",
    "n_ctx=2048,\n",
    "n_gpu_layers=-1,\n",
    "n_batch=512,\n",
    "callback_manager=callback_manager,\n",
    "verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is first day of week?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      30.11 ms /   100 runs   (    0.30 ms per token,  3321.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     524.39 ms /     8 tokens (   65.55 ms per token,    15.26 tokens per second)\n",
      "llama_print_timings:        eval time =    7763.41 ms /    99 runs   (   78.42 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =    8777.73 ms /   107 tokens\n"
     ]
    }
   ],
   "source": [
    "output = llm(question, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-8a4d6f8f-3e77-4555-ba87-c27aa2204de9',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1709487985,\n",
       " 'model': '/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf',\n",
       " 'choices': [{'text': \"\\n\\nThe first day of the week is traditionally considered to be Sunday, according to both the Christian and the Jewish calendars. This is based on the biblical account of creation in the Book of Genesis, where God rested on the seventh day (Saturday) after having created the world in six days. As a result, Sunday became known as the 'Lord's Day', a day of rest and worship. However, it's important to note that different cultures and\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 8, 'completion_tokens': 100, 'total_tokens': 108}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline and tokenizer once\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "def extract_text_triplets(input_text):\n",
    "    \"\"\"\n",
    "    Extracts triplets from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The text from which to extract triplets.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each representing a triplet with 'head', 'type', and 'tail'.\n",
    "    \"\"\"\n",
    "    # Use the tokenizer manually since we need special tokens\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode([\n",
    "        triplet_extractor(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]\n",
    "    ])\n",
    "\n",
    "    # Function to parse the generated text and extract the triplets\n",
    "    def extract_triplets(text):\n",
    "        triplets = []\n",
    "        relation, subject, object_ = '', '', ''\n",
    "        text = text.strip()\n",
    "        current = 'x'\n",
    "        for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "            if token == \"<triplet>\":\n",
    "                current = 't'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                    relation = ''\n",
    "                subject = ''\n",
    "            elif token == \"<subj>\":\n",
    "                current = 's'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                object_ = ''\n",
    "            elif token == \"<obj>\":\n",
    "                current = 'o'\n",
    "                relation = ''\n",
    "            else:\n",
    "                if current == 't':\n",
    "                    subject += ' ' + token\n",
    "                elif current == 's':\n",
    "                    object_ += ' ' + token\n",
    "                elif current == 'o':\n",
    "                    relation += ' ' + token\n",
    "        if subject != '' and relation != '' and object_ != '':\n",
    "            triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "        return triplets\n",
    "\n",
    "    extracted_triplets = extract_triplets(extracted_text[0])\n",
    "    return extracted_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\"\n",
    "extracted_triplets = extract_text_triplets(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': 'Punta Cana',\n",
       "  'type': 'located in the administrative territorial entity',\n",
       "  'tail': 'La Altagracia Province'},\n",
       " {'head': 'Punta Cana', 'type': 'country', 'tail': 'Dominican Republic'},\n",
       " {'head': 'Higuey',\n",
       "  'type': 'located in the administrative territorial entity',\n",
       "  'tail': 'La Altagracia Province'},\n",
       " {'head': 'Higuey', 'type': 'country', 'tail': 'Dominican Republic'},\n",
       " {'head': 'La Altagracia Province',\n",
       "  'type': 'country',\n",
       "  'tail': 'Dominican Republic'},\n",
       " {'head': 'Dominican Republic',\n",
       "  'type': 'contains administrative territorial entity',\n",
       "  'tail': 'La Altagracia Province'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''<s>[INST] <<SYS>>\n",
    "Assistant is an expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to trigger actions for User by responding with JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "The available action to Assistant is:\n",
    "\n",
    "- \"extract_text_triplets\": Useful for when Assistant is asked to extract triplets from a given text.\n",
    "  - To use the extract_triplets tool, Assistant should respond like so:\n",
    "    {{\"action\": \"extract_text_triplets\", \"action_input\": \"Your text here\"}}\n",
    "\n",
    "\n",
    "Here are the entities and relations and their corresponding mappings.\n",
    "\"entities\": {\"Loc\": {\"short\": \"Loc\", \"verbose\": \"Location\"}, \"Org\": {\"short\": \"Org\", \"verbose\": \"Organization\"}, \"Peop\": {\"short\": \"Peop\", \"verbose\":\"People\"}, \"Other\": {\"short\": \"Other\", \"verbose\": \"Other\"}}, \n",
    "\"relations\": {\"Work_For\": {\"short\": \"Work\", \"verbose\": \"Work for\", \"symmetric\": false}, \"Kill\": {\"short\": \"Kill\", \"verbose\": \"Kill\", \"symmetric\": false}, \"OrgBased_In\": {\"short\": \"OrgBI\", \"verbose\": \"Organization based in\", \"symmetric\": false}, \"Live_In\": {\"short\": \"Live\", \"verbose\": \"Live in\", \"symmetric\": false}, \"Located_In\": {\"short\": \"LocIn\", \"verbose\": \"Located in\", \"symmetric\": false}}}\n",
    "mapping = {'Kill': 'killed by', 'Live_In': 'residence', 'Located_In': 'location', 'OrgBased_In': 'headquarters location', 'Work_For': 'employer'}\n",
    "mapping_types = {'Peop': '<peop>', 'Org': '<org>', 'Other': '<other>', 'Loc': '<loc>'}\n",
    "\n",
    "When extracting triples make sure that head and tail entities are from mapping dictionary and relations are from mapping_types dictionary only\n",
    "\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: I'm good thanks, how are you?\n",
    "User: Can you extract all the triplets from this text: \"Gràcia is a district of the city of Barcelona, Spain.\"\n",
    "Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"Gràcia is a district of the city of Barcelona, Spain.\"}}\n",
    "User: Also give triples for \"obama was US president\"\n",
    "Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"obama was US president\"}}\n",
    "\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{0}[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_command1(command):\n",
    "    # Put user command into prompt\n",
    "    prompt = prompt_template.format(\"User: \" + command)\n",
    "    # Send command to the model\n",
    "    output = llm(prompt, max_tokens=2000, stop=[\"User:\"])\n",
    "    response = output['choices'][0]['text']\n",
    "    #print(type(response))\n",
    "    firstBracketIndex = response.index(\"{\")\n",
    "    lastBracketIndex = len(response) - response[::-1].index(\"}\")\n",
    "    print(firstBracketIndex)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       7.28 ms /    26 runs   (    0.28 ms per token,  3573.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2541.63 ms /    26 runs   (   97.75 ms per token,    10.23 tokens per second)\n",
      "llama_print_timings:       total time =    2691.16 ms /    27 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' {\"action\": \"extract_text_triplets\", \"action_input\": \"Ananya is a good girl\"}'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command1(\"Can you please give triple for \\\"Ananya is a good girl\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_command(command):\n",
    "    # Put user command into prompt\n",
    "    prompt = prompt_template.format(\"User: \" + command)\n",
    "    # Send command to the model\n",
    "    output = llm(prompt, max_tokens=2000, stop=[\"User:\"])\n",
    "    response = output['choices'][0]['text']\n",
    "\n",
    "    # try to find json in the response\n",
    "    try:\n",
    "        # Extract json from model response by finding first and last brackets {}\n",
    "        firstBracketIndex = response.index(\"{\")\n",
    "        lastBracketIndex = len(response) - response[::-1].index(\"}\")\n",
    "        jsonString = response[firstBracketIndex:lastBracketIndex]\n",
    "        responseJson = json.loads(jsonString)\n",
    "        if responseJson['action'] == 'extract_text_triplets':\n",
    "            extracted_triplets = extract_text_triplets(responseJson['action_input'])\n",
    "            return extracted_triplets   \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # No json match, just return response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       6.08 ms /    26 runs   (    0.23 ms per token,  4277.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2339.56 ms /    26 runs   (   89.98 ms per token,    11.11 tokens per second)\n",
      "llama_print_timings:       total time =    2447.47 ms /    27 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'Ananya', 'type': 'instance of', 'tail': 'good girl'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Can you please give triple for \\\"Ananya is a good girl\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    26 runs   (    0.20 ms per token,  4961.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     847.20 ms /    16 tokens (   52.95 ms per token,    18.89 tokens per second)\n",
      "llama_print_timings:        eval time =    2072.23 ms /    25 runs   (   82.89 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =    3088.59 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello_op\n",
      "Extracted triples have been saved to out.json\n"
     ]
    }
   ],
   "source": [
    "extracted_triplets = process_command(\"Can you please give triple for \\\"Ananya is a good girl\\\"\")\n",
    "\n",
    "# Save the extracted triples to a JSON file\n",
    "output_file_path = 'out.json'  # Define the output file path\n",
    "\n",
    "# Write the extracted triples to the output file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    json.dump(extracted_triplets, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the file has been saved\n",
    "print(f\"Extracted triples have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3500.53 ms\n",
      "llama_print_timings:      sample time =      15.87 ms /    56 runs   (    0.28 ms per token,  3528.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3499.73 ms /   335 tokens (   10.45 ms per token,    95.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4712.87 ms /    55 runs   (   85.69 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =    8463.30 ms /   390 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'Punta Cana',\n",
       "  'type': 'located in the administrative territorial entity',\n",
       "  'tail': 'La Altagracia Province'},\n",
       " {'head': 'Punta Cana', 'type': 'country', 'tail': 'Dominican Republic'},\n",
       " {'head': 'Higuey',\n",
       "  'type': 'located in the administrative territorial entity',\n",
       "  'tail': 'La Altagracia Province'},\n",
       " {'head': 'Higuey', 'type': 'country', 'tail': 'Dominican Republic'},\n",
       " {'head': 'La Altagracia Province',\n",
       "  'type': 'country',\n",
       "  'tail': 'Dominican Republic'},\n",
       " {'head': 'Dominican Republic',\n",
       "  'type': 'contains administrative territorial entity',\n",
       "  'tail': 'La Altagracia Province'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Can you give triple for: Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Twenty-five years after the assassination of President John F. Kennedy , Lee Harvey Oswald 's widow says she now believes Oswald did not act alone in the killing .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"Can you please give triple for \\\"{context}\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3500.53 ms\n",
      "llama_print_timings:      sample time =      19.74 ms /    60 runs   (    0.33 ms per token,  3039.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     829.69 ms /    49 tokens (   16.93 ms per token,    59.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4673.44 ms /    59 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    5842.80 ms /   108 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'assassination of President John F. Kennedy',\n",
       "  'type': 'participant',\n",
       "  'tail': 'Lee Harvey Oswald'},\n",
       " {'head': 'Lee Harvey Oswald',\n",
       "  'type': 'significant event',\n",
       "  'tail': 'assassination of President John F. Kennedy'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      20.31 ms /    60 runs   (    0.34 ms per token,  2953.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     817.35 ms /    45 tokens (   18.16 ms per token,    55.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4720.26 ms /    59 runs   (   80.00 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =    5852.14 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.52 ms /    60 runs   (    0.33 ms per token,  3074.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     666.03 ms /    44 tokens (   15.14 ms per token,    66.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4699.43 ms /    59 runs   (   79.65 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =    5667.70 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      10.36 ms /    36 runs   (    0.29 ms per token,  3476.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     404.14 ms /    20 tokens (   20.21 ms per token,    49.49 tokens per second)\n",
      "llama_print_timings:        eval time =    2790.31 ms /    35 runs   (   79.72 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =    3351.33 ms /    55 tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract triples for each context in eval.json and create pred.json\n",
    "def generate_pred_json(eval_file_path, pred_file_path):\n",
    "    # Load the evaluation data from eval.json\n",
    "    with open(eval_file_path, 'r') as file:\n",
    "        eval_data = json.load(file)\n",
    "    \n",
    "    # Initialize a list to hold the modified data with extracted triples\n",
    "    modified_data = []\n",
    "    \n",
    "    # Iterate over each item in the evaluation data\n",
    "    for item in eval_data:\n",
    "        context = item['context']\n",
    "        # Prepare the command with the context\n",
    "        command = f\"Can you please give triple for \\\"{context}\\\"\"\n",
    "        # Use the process_command function to predict the extracted triples\n",
    "        extracted_triplets = process_command(command)\n",
    "        # Append the extracted triples to the item under the 'triples' key\n",
    "        item['triples'] = extracted_triplets\n",
    "        # Append the modified item to the modified_data list\n",
    "        modified_data.append(item)\n",
    "    \n",
    "    # Write the modified data with extracted triples to pred.json\n",
    "    with open(pred_file_path, 'w') as file:\n",
    "        json.dump(modified_data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "eval_file_path = '/Users/ananyahooda/Desktop/Text2kg/shot_col.json' # Replace with the actual path to your eval.json file\n",
    "pred_file_path = '/Users/ananyahooda/Desktop/Text2kg/out.json' # The output file path\n",
    "generate_pred_json(eval_file_path, pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      15.49 ms /    51 runs   (    0.30 ms per token,  3291.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     810.37 ms /    35 tokens (   23.15 ms per token,    43.19 tokens per second)\n",
      "llama_print_timings:        eval time =    3986.93 ms /    50 runs   (   79.74 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =    5033.26 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      81.57 ms /   217 runs   (    0.38 ms per token,  2660.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     387.39 ms /    13 tokens (   29.80 ms per token,    33.56 tokens per second)\n",
      "llama_print_timings:        eval time =   17097.64 ms /   216 runs   (   79.16 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   18798.52 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      39.72 ms /    78 runs   (    0.51 ms per token,  1963.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     953.06 ms /    67 tokens (   14.22 ms per token,    70.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6125.62 ms /    77 runs   (   79.55 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    7520.98 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      16.99 ms /    75 runs   (    0.23 ms per token,  4414.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     698.04 ms /    61 tokens (   11.44 ms per token,    87.39 tokens per second)\n",
      "llama_print_timings:        eval time =    6329.95 ms /    74 runs   (   85.54 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =    7289.97 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      85.45 ms /   245 runs   (    0.35 ms per token,  2867.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     891.13 ms /    76 tokens (   11.73 ms per token,    85.29 tokens per second)\n",
      "llama_print_timings:        eval time =   19992.01 ms /   244 runs   (   81.93 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:       total time =   22193.08 ms /   320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.76 ms /    69 runs   (    0.33 ms per token,  3032.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     682.37 ms /    53 tokens (   12.87 ms per token,    77.67 tokens per second)\n",
      "llama_print_timings:        eval time =    5361.03 ms /    68 runs   (   78.84 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =    6407.58 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      24.71 ms /    72 runs   (    0.34 ms per token,  2914.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     671.10 ms /    56 tokens (   11.98 ms per token,    83.44 tokens per second)\n",
      "llama_print_timings:        eval time =    5888.11 ms /    71 runs   (   82.93 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =    7173.75 ms /   127 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      14.85 ms /    56 runs   (    0.27 ms per token,  3772.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1160.24 ms /    39 tokens (   29.75 ms per token,    33.61 tokens per second)\n",
      "llama_print_timings:        eval time =    5070.24 ms /    55 runs   (   92.19 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =    6501.57 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      20.07 ms /    57 runs   (    0.35 ms per token,  2839.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     677.91 ms /    45 tokens (   15.06 ms per token,    66.38 tokens per second)\n",
      "llama_print_timings:        eval time =    4453.66 ms /    56 runs   (   79.53 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    5443.77 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.67 ms /    52 runs   (    0.38 ms per token,  2643.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     666.04 ms /    36 tokens (   18.50 ms per token,    54.05 tokens per second)\n",
      "llama_print_timings:        eval time =    3987.33 ms /    51 runs   (   78.18 ms per token,    12.79 tokens per second)\n",
      "llama_print_timings:       total time =    4931.95 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       9.39 ms /    28 runs   (    0.34 ms per token,  2980.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     389.94 ms /    13 tokens (   30.00 ms per token,    33.34 tokens per second)\n",
      "llama_print_timings:        eval time =    2117.02 ms /    27 runs   (   78.41 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =    2656.91 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       9.68 ms /    27 runs   (    0.36 ms per token,  2788.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     392.45 ms /    13 tokens (   30.19 ms per token,    33.12 tokens per second)\n",
      "llama_print_timings:        eval time =    2030.59 ms /    26 runs   (   78.10 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:       total time =    2568.61 ms /    39 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      30.12 ms /    90 runs   (    0.33 ms per token,  2987.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     391.41 ms /    13 tokens (   30.11 ms per token,    33.21 tokens per second)\n",
      "llama_print_timings:        eval time =    7007.14 ms /    89 runs   (   78.73 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:       total time =    7858.54 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      62.42 ms /   163 runs   (    0.38 ms per token,  2611.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     318.34 ms /    30 tokens (   10.61 ms per token,    94.24 tokens per second)\n",
      "llama_print_timings:        eval time =   13059.39 ms /   162 runs   (   80.61 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =   14293.15 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.82 ms /    31 runs   (    0.64 ms per token,  1564.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.60 ms /    15 tokens (   26.77 ms per token,    37.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2395.61 ms /    30 runs   (   79.85 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =    3033.20 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      37.83 ms /    60 runs   (    0.63 ms per token,  1586.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     400.88 ms /    11 tokens (   36.44 ms per token,    27.44 tokens per second)\n",
      "llama_print_timings:        eval time =    4760.99 ms /    59 runs   (   80.69 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =    5624.21 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      51.99 ms /    82 runs   (    0.63 ms per token,  1577.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     944.08 ms /    66 tokens (   14.30 ms per token,    69.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6488.97 ms /    81 runs   (   80.11 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    8070.37 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      30.59 ms /    65 runs   (    0.47 ms per token,  2124.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     682.79 ms /    49 tokens (   13.93 ms per token,    71.76 tokens per second)\n",
      "llama_print_timings:        eval time =    5408.23 ms /    64 runs   (   84.50 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    6476.35 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       9.21 ms /    46 runs   (    0.20 ms per token,  4997.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     600.20 ms /    30 tokens (   20.01 ms per token,    49.98 tokens per second)\n",
      "llama_print_timings:        eval time =    3710.36 ms /    45 runs   (   82.45 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    4456.70 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    28 runs   (    0.17 ms per token,  5903.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     432.84 ms /    13 tokens (   33.30 ms per token,    30.03 tokens per second)\n",
      "llama_print_timings:        eval time =    2286.69 ms /    27 runs   (   84.69 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:       total time =    2791.48 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      27.33 ms /   116 runs   (    0.24 ms per token,  4244.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     665.20 ms /    39 tokens (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_print_timings:        eval time =    9541.14 ms /   115 runs   (   82.97 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   10667.30 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /    43 runs   (    0.35 ms per token,  2873.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     496.23 ms /    31 tokens (   16.01 ms per token,    62.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3456.55 ms /    42 runs   (   82.30 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    4339.16 ms /    73 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      49.42 ms /   148 runs   (    0.33 ms per token,  2994.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     658.09 ms /    39 tokens (   16.87 ms per token,    59.26 tokens per second)\n",
      "llama_print_timings:        eval time =   11484.15 ms /   147 runs   (   78.12 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:       total time =   12931.52 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      10.91 ms /    56 runs   (    0.19 ms per token,  5134.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     659.80 ms /    42 tokens (   15.71 ms per token,    63.66 tokens per second)\n",
      "llama_print_timings:        eval time =    5078.85 ms /    55 runs   (   92.34 ms per token,    10.83 tokens per second)\n",
      "llama_print_timings:       total time =    5919.74 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      11.18 ms /    55 runs   (    0.20 ms per token,  4920.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     660.81 ms /    39 tokens (   16.94 ms per token,    59.02 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.70 ms /    54 runs   (   83.99 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =    5370.89 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      17.06 ms /    62 runs   (    0.28 ms per token,  3633.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     675.40 ms /    47 tokens (   14.37 ms per token,    69.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4939.18 ms /    61 runs   (   80.97 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    5889.09 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      45.74 ms /   157 runs   (    0.29 ms per token,  3432.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     745.09 ms /    35 tokens (   21.29 ms per token,    46.97 tokens per second)\n",
      "llama_print_timings:        eval time =   12765.94 ms /   156 runs   (   81.83 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   14279.19 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      56.56 ms /   183 runs   (    0.31 ms per token,  3235.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     673.47 ms /    49 tokens (   13.74 ms per token,    72.76 tokens per second)\n",
      "llama_print_timings:        eval time =   14529.74 ms /   182 runs   (   79.83 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =   16198.63 ms /   231 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      15.83 ms /    58 runs   (    0.27 ms per token,  3663.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     663.43 ms /    42 tokens (   15.80 ms per token,    63.31 tokens per second)\n",
      "llama_print_timings:        eval time =    4677.84 ms /    57 runs   (   82.07 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    5603.93 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      15.06 ms /    53 runs   (    0.28 ms per token,  3519.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     690.54 ms /    37 tokens (   18.66 ms per token,    53.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4239.75 ms /    52 runs   (   81.53 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    5169.56 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /    54 runs   (    0.26 ms per token,  3884.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     764.23 ms /    42 tokens (   18.20 ms per token,    54.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4385.10 ms /    53 runs   (   82.74 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =    5354.24 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      14.16 ms /    50 runs   (    0.28 ms per token,  3530.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     669.32 ms /    34 tokens (   19.69 ms per token,    50.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4044.04 ms /    49 runs   (   82.53 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =    4944.64 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      12.27 ms /    62 runs   (    0.20 ms per token,  5052.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     765.64 ms /    46 tokens (   16.64 ms per token,    60.08 tokens per second)\n",
      "llama_print_timings:        eval time =    5172.91 ms /    61 runs   (   84.80 ms per token,    11.79 tokens per second)\n",
      "llama_print_timings:       total time =    6131.63 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.81 ms /    81 runs   (    0.24 ms per token,  4089.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     939.12 ms /    65 tokens (   14.45 ms per token,    69.21 tokens per second)\n",
      "llama_print_timings:        eval time =    6681.87 ms /    80 runs   (   83.52 ms per token,    11.97 tokens per second)\n",
      "llama_print_timings:       total time =    7929.17 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      12.09 ms /    58 runs   (    0.21 ms per token,  4798.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     686.51 ms /    42 tokens (   16.35 ms per token,    61.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4792.10 ms /    57 runs   (   84.07 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =    5671.08 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.14 ms /    82 runs   (    0.23 ms per token,  4283.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     977.81 ms /    66 tokens (   14.82 ms per token,    67.50 tokens per second)\n",
      "llama_print_timings:        eval time =    7084.12 ms /    81 runs   (   87.46 ms per token,    11.43 tokens per second)\n",
      "llama_print_timings:       total time =    8386.85 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      57.06 ms /   144 runs   (    0.40 ms per token,  2523.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     702.07 ms /    45 tokens (   15.60 ms per token,    64.10 tokens per second)\n",
      "llama_print_timings:        eval time =   11442.90 ms /   143 runs   (   80.02 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =   13063.47 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      20.90 ms /    62 runs   (    0.34 ms per token,  2966.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.45 ms /    46 tokens (   14.77 ms per token,    67.70 tokens per second)\n",
      "llama_print_timings:        eval time =    5067.08 ms /    61 runs   (   83.07 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =    6064.69 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      79.58 ms /   240 runs   (    0.33 ms per token,  3015.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     973.43 ms /    91 tokens (   10.70 ms per token,    93.48 tokens per second)\n",
      "llama_print_timings:        eval time =   20690.07 ms /   239 runs   (   86.57 ms per token,    11.55 tokens per second)\n",
      "llama_print_timings:       total time =   23146.48 ms /   330 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       9.31 ms /    29 runs   (    0.32 ms per token,  3115.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.92 ms /    13 tokens (   31.38 ms per token,    31.87 tokens per second)\n",
      "llama_print_timings:        eval time =    2367.85 ms /    28 runs   (   84.57 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    2928.69 ms /    41 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       9.40 ms /    30 runs   (    0.31 ms per token,  3192.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     444.52 ms /    15 tokens (   29.63 ms per token,    33.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2755.16 ms /    29 runs   (   95.01 ms per token,    10.53 tokens per second)\n",
      "llama_print_timings:       total time =    3344.42 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.48 ms /    62 runs   (    0.31 ms per token,  3182.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     886.21 ms /    46 tokens (   19.27 ms per token,    51.91 tokens per second)\n",
      "llama_print_timings:        eval time =    7174.59 ms /    61 runs   (  117.62 ms per token,     8.50 tokens per second)\n",
      "llama_print_timings:       total time =    8360.91 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      50.00 ms /   144 runs   (    0.35 ms per token,  2880.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1101.57 ms /    51 tokens (   21.60 ms per token,    46.30 tokens per second)\n",
      "llama_print_timings:        eval time =   25852.83 ms /   143 runs   (  180.79 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:       total time =   27767.31 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      34.56 ms /    70 runs   (    0.49 ms per token,  2025.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1015.48 ms /    59 tokens (   17.21 ms per token,    58.10 tokens per second)\n",
      "llama_print_timings:        eval time =   15106.26 ms /    69 runs   (  218.93 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:       total time =   16559.28 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      57.72 ms /   142 runs   (    0.41 ms per token,  2460.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1299.52 ms /    54 tokens (   24.07 ms per token,    41.55 tokens per second)\n",
      "llama_print_timings:        eval time =   29332.63 ms /   141 runs   (  208.03 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =   31546.32 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.84 ms /    64 runs   (    0.31 ms per token,  3226.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1117.00 ms /    48 tokens (   23.27 ms per token,    42.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9481.36 ms /    63 runs   (  150.50 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:       total time =   10923.26 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      20.77 ms /    67 runs   (    0.31 ms per token,  3225.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     889.29 ms /    55 tokens (   16.17 ms per token,    61.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9658.97 ms /    66 runs   (  146.35 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:       total time =   10885.38 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      15.72 ms /    56 runs   (    0.28 ms per token,  3562.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     985.44 ms /    40 tokens (   24.64 ms per token,    40.59 tokens per second)\n",
      "llama_print_timings:        eval time =    7429.65 ms /    55 runs   (  135.08 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:       total time =    8677.97 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      87.14 ms /   190 runs   (    0.46 ms per token,  2180.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     908.76 ms /    45 tokens (   20.19 ms per token,    49.52 tokens per second)\n",
      "llama_print_timings:        eval time =   25474.81 ms /   189 runs   (  134.79 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:       total time =   27644.68 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      50.53 ms /    66 runs   (    0.77 ms per token,  1306.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     929.49 ms /    51 tokens (   18.23 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6967.50 ms /    65 runs   (  107.19 ms per token,     9.33 tokens per second)\n",
      "llama_print_timings:       total time =    8487.73 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     132.19 ms /   250 runs   (    0.53 ms per token,  1891.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1242.37 ms /    88 tokens (   14.12 ms per token,    70.83 tokens per second)\n",
      "llama_print_timings:        eval time =   28034.89 ms /   249 runs   (  112.59 ms per token,     8.88 tokens per second)\n",
      "llama_print_timings:       total time =   31084.68 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      34.50 ms /    52 runs   (    0.66 ms per token,  1507.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     720.97 ms /    36 tokens (   20.03 ms per token,    49.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5573.08 ms /    51 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_print_timings:       total time =    6702.69 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      41.79 ms /    53 runs   (    0.79 ms per token,  1268.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     827.00 ms /    37 tokens (   22.35 ms per token,    44.74 tokens per second)\n",
      "llama_print_timings:        eval time =    5023.70 ms /    52 runs   (   96.61 ms per token,    10.35 tokens per second)\n",
      "llama_print_timings:       total time =    6322.71 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     167.75 ms /   207 runs   (    0.81 ms per token,  1233.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     790.26 ms /    48 tokens (   16.46 ms per token,    60.74 tokens per second)\n",
      "llama_print_timings:        eval time =   20851.03 ms /   206 runs   (  101.22 ms per token,     9.88 tokens per second)\n",
      "llama_print_timings:       total time =   23616.36 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     128.61 ms /   159 runs   (    0.81 ms per token,  1236.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     419.38 ms /    29 tokens (   14.46 ms per token,    69.15 tokens per second)\n",
      "llama_print_timings:        eval time =   15738.68 ms /   158 runs   (   99.61 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =   17562.54 ms /   187 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      43.62 ms /    56 runs   (    0.78 ms per token,  1283.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     785.75 ms /    40 tokens (   19.64 ms per token,    50.91 tokens per second)\n",
      "llama_print_timings:        eval time =    5275.08 ms /    55 runs   (   95.91 ms per token,    10.43 tokens per second)\n",
      "llama_print_timings:       total time =    6570.28 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      42.87 ms /    51 runs   (    0.84 ms per token,  1189.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     872.31 ms /    35 tokens (   24.92 ms per token,    40.12 tokens per second)\n",
      "llama_print_timings:        eval time =    4889.57 ms /    50 runs   (   97.79 ms per token,    10.23 tokens per second)\n",
      "llama_print_timings:       total time =    6224.57 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      39.68 ms /    51 runs   (    0.78 ms per token,  1285.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     743.58 ms /    35 tokens (   21.25 ms per token,    47.07 tokens per second)\n",
      "llama_print_timings:        eval time =    4650.17 ms /    50 runs   (   93.00 ms per token,    10.75 tokens per second)\n",
      "llama_print_timings:       total time =    5836.81 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     150.46 ms /   189 runs   (    0.80 ms per token,  1256.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.79 ms /    47 tokens (   16.04 ms per token,    62.35 tokens per second)\n",
      "llama_print_timings:        eval time =   18229.25 ms /   188 runs   (   96.96 ms per token,    10.31 tokens per second)\n",
      "llama_print_timings:       total time =   20801.58 ms /   235 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     106.88 ms /   130 runs   (    0.82 ms per token,  1216.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     735.16 ms /    36 tokens (   20.42 ms per token,    48.97 tokens per second)\n",
      "llama_print_timings:        eval time =   11597.66 ms /   129 runs   (   89.90 ms per token,    11.12 tokens per second)\n",
      "llama_print_timings:       total time =   13568.34 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      58.86 ms /    73 runs   (    0.81 ms per token,  1240.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     681.48 ms /    57 tokens (   11.96 ms per token,    83.64 tokens per second)\n",
      "llama_print_timings:        eval time =    6647.43 ms /    72 runs   (   92.33 ms per token,    10.83 tokens per second)\n",
      "llama_print_timings:       total time =    7987.88 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      68.12 ms /    92 runs   (    0.74 ms per token,  1350.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     723.64 ms /    39 tokens (   18.55 ms per token,    53.89 tokens per second)\n",
      "llama_print_timings:        eval time =    8183.58 ms /    91 runs   (   89.93 ms per token,    11.12 tokens per second)\n",
      "llama_print_timings:       total time =    9658.83 ms /   130 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      32.74 ms /    45 runs   (    0.73 ms per token,  1374.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     429.95 ms /    29 tokens (   14.83 ms per token,    67.45 tokens per second)\n",
      "llama_print_timings:        eval time =    3678.21 ms /    44 runs   (   83.60 ms per token,    11.96 tokens per second)\n",
      "llama_print_timings:       total time =    4545.29 ms /    73 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      26.23 ms /    30 runs   (    0.87 ms per token,  1143.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     435.98 ms /    15 tokens (   29.07 ms per token,    34.41 tokens per second)\n",
      "llama_print_timings:        eval time =    2504.52 ms /    29 runs   (   86.36 ms per token,    11.58 tokens per second)\n",
      "llama_print_timings:       total time =    3213.50 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      42.56 ms /    57 runs   (    0.75 ms per token,  1339.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     735.95 ms /    41 tokens (   17.95 ms per token,    55.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4955.58 ms /    56 runs   (   88.49 ms per token,    11.30 tokens per second)\n",
      "llama_print_timings:       total time =    6214.53 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      71.27 ms /   171 runs   (    0.42 ms per token,  2399.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     668.01 ms /    45 tokens (   14.84 ms per token,    67.36 tokens per second)\n",
      "llama_print_timings:        eval time =   16331.96 ms /   170 runs   (   96.07 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =   18005.31 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      16.12 ms /    56 runs   (    0.29 ms per token,  3474.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     752.24 ms /    40 tokens (   18.81 ms per token,    53.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4615.50 ms /    55 runs   (   83.92 ms per token,    11.92 tokens per second)\n",
      "llama_print_timings:       total time =    5627.43 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      12.55 ms /    52 runs   (    0.24 ms per token,  4144.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     686.62 ms /    39 tokens (   17.61 ms per token,    56.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4814.99 ms /    51 runs   (   94.41 ms per token,    10.59 tokens per second)\n",
      "llama_print_timings:       total time =    5685.10 ms /    90 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      17.75 ms /    68 runs   (    0.26 ms per token,  3830.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     746.53 ms /    52 tokens (   14.36 ms per token,    69.66 tokens per second)\n",
      "llama_print_timings:        eval time =    6396.37 ms /    67 runs   (   95.47 ms per token,    10.47 tokens per second)\n",
      "llama_print_timings:       total time =    7414.39 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       7.37 ms /    29 runs   (    0.25 ms per token,  3934.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.93 ms /    14 tokens (   27.92 ms per token,    35.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2310.47 ms /    28 runs   (   82.52 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =    2817.57 ms /    42 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      16.42 ms /    53 runs   (    0.31 ms per token,  3228.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     677.02 ms /    37 tokens (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:        eval time =    4916.94 ms /    52 runs   (   94.56 ms per token,    10.58 tokens per second)\n",
      "llama_print_timings:       total time =    5846.49 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     277.10 ms /   397 runs   (    0.70 ms per token,  1432.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2204.48 ms /   173 tokens (   12.74 ms per token,    78.48 tokens per second)\n",
      "llama_print_timings:        eval time =   34001.94 ms /   396 runs   (   85.86 ms per token,    11.65 tokens per second)\n",
      "llama_print_timings:       total time =   39796.26 ms /   569 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      83.47 ms /   108 runs   (    0.77 ms per token,  1293.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     668.26 ms /    41 tokens (   16.30 ms per token,    61.35 tokens per second)\n",
      "llama_print_timings:        eval time =    8667.57 ms /   107 runs   (   81.01 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =   10301.17 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 44 (char 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     240.03 ms /   301 runs   (    0.80 ms per token,  1254.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     728.66 ms /    54 tokens (   13.49 ms per token,    74.11 tokens per second)\n",
      "llama_print_timings:        eval time =   24584.80 ms /   300 runs   (   81.95 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:       total time =   28234.06 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      52.08 ms /    63 runs   (    0.83 ms per token,  1209.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     671.60 ms /    47 tokens (   14.29 ms per token,    69.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4877.33 ms /    62 runs   (   78.67 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =    6120.80 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     158.34 ms /   208 runs   (    0.76 ms per token,  1313.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     933.47 ms /    76 tokens (   12.28 ms per token,    81.42 tokens per second)\n",
      "llama_print_timings:        eval time =   17244.85 ms /   207 runs   (   83.31 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =   20051.82 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      63.58 ms /    91 runs   (    0.70 ms per token,  1431.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.40 ms /    12 tokens (   33.45 ms per token,    29.90 tokens per second)\n",
      "llama_print_timings:        eval time =    7464.03 ms /    90 runs   (   82.93 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =    8621.28 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      25.65 ms /    40 runs   (    0.64 ms per token,  1559.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     409.35 ms /    25 tokens (   16.37 ms per token,    61.07 tokens per second)\n",
      "llama_print_timings:        eval time =    3164.43 ms /    39 runs   (   81.14 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =    3867.41 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      46.25 ms /    58 runs   (    0.80 ms per token,  1253.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     668.10 ms /    46 tokens (   14.52 ms per token,    68.85 tokens per second)\n",
      "llama_print_timings:        eval time =    4667.74 ms /    57 runs   (   81.89 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =    5845.63 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     186.88 ms /   234 runs   (    0.80 ms per token,  1252.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     668.88 ms /    64 tokens (   10.45 ms per token,    95.68 tokens per second)\n",
      "llama_print_timings:        eval time =   19065.84 ms /   233 runs   (   81.83 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   22050.03 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     131.99 ms /   167 runs   (    0.79 ms per token,  1265.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     697.36 ms /    63 tokens (   11.07 ms per token,    90.34 tokens per second)\n",
      "llama_print_timings:        eval time =   13770.58 ms /   166 runs   (   82.96 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   15950.69 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      72.41 ms /   148 runs   (    0.49 ms per token,  2043.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.91 ms /    45 tokens (   15.46 ms per token,    64.66 tokens per second)\n",
      "llama_print_timings:        eval time =   12418.41 ms /   147 runs   (   84.48 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =   14072.40 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      73.15 ms /   107 runs   (    0.68 ms per token,  1462.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     662.52 ms /    46 tokens (   14.40 ms per token,    69.43 tokens per second)\n",
      "llama_print_timings:        eval time =    8677.66 ms /   106 runs   (   81.86 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   10219.47 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 222 (char 221)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      58.66 ms /    71 runs   (    0.83 ms per token,  1210.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     703.24 ms /    55 tokens (   12.79 ms per token,    78.21 tokens per second)\n",
      "llama_print_timings:        eval time =    5607.58 ms /    70 runs   (   80.11 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    6958.24 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      60.49 ms /    72 runs   (    0.84 ms per token,  1190.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     678.00 ms /    56 tokens (   12.11 ms per token,    82.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5608.38 ms /    71 runs   (   78.99 ms per token,    12.66 tokens per second)\n",
      "llama_print_timings:       total time =    6947.33 ms /   127 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      28.90 ms /    73 runs   (    0.40 ms per token,  2526.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     668.19 ms /    59 tokens (   11.33 ms per token,    88.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6158.72 ms /    72 runs   (   85.54 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =    7189.49 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.70 ms /    61 runs   (    0.37 ms per token,  2686.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     792.87 ms /    45 tokens (   17.62 ms per token,    56.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4743.44 ms /    60 runs   (   79.06 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    5890.53 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      19.04 ms /    53 runs   (    0.36 ms per token,  2783.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     662.73 ms /    36 tokens (   18.41 ms per token,    54.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4073.44 ms /    52 runs   (   78.34 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =    5018.48 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      27.65 ms /    76 runs   (    0.36 ms per token,  2748.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     683.30 ms /    62 tokens (   11.02 ms per token,    90.74 tokens per second)\n",
      "llama_print_timings:        eval time =    5941.11 ms /    75 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    7054.57 ms /   137 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.11 ms /    63 runs   (    0.35 ms per token,  2849.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     664.02 ms /    48 tokens (   13.83 ms per token,    72.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4863.22 ms /    62 runs   (   78.44 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =    5879.40 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      21.00 ms /    57 runs   (    0.37 ms per token,  2714.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     657.37 ms /    42 tokens (   15.65 ms per token,    63.89 tokens per second)\n",
      "llama_print_timings:        eval time =    4389.88 ms /    56 runs   (   78.39 ms per token,    12.76 tokens per second)\n",
      "llama_print_timings:       total time =    5360.19 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      41.62 ms /   125 runs   (    0.33 ms per token,  3003.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     669.03 ms /    43 tokens (   15.56 ms per token,    64.27 tokens per second)\n",
      "llama_print_timings:        eval time =   10270.45 ms /   124 runs   (   82.83 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   11597.01 ms /   167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      25.90 ms /    70 runs   (    0.37 ms per token,  2702.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     680.53 ms /    54 tokens (   12.60 ms per token,    79.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5387.46 ms /    69 runs   (   78.08 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =    6470.66 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      24.48 ms /    77 runs   (    0.32 ms per token,  3144.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.22 ms /    61 tokens (    9.66 ms per token,   103.53 tokens per second)\n",
      "llama_print_timings:        eval time =    5961.95 ms /    76 runs   (   78.45 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =    6932.39 ms /   137 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      30.82 ms /   129 runs   (    0.24 ms per token,  4185.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.83 ms /    28 tokens (   26.71 ms per token,    37.44 tokens per second)\n",
      "llama_print_timings:        eval time =   10550.53 ms /   128 runs   (   82.43 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =   11797.00 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      12.69 ms /    76 runs   (    0.17 ms per token,  5988.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     687.87 ms /    62 tokens (   11.09 ms per token,    90.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6240.72 ms /    75 runs   (   83.21 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =    7117.62 ms /   137 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      20.65 ms /    47 runs   (    0.44 ms per token,  2276.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.99 ms /    31 tokens (   13.16 ms per token,    75.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4036.16 ms /    46 runs   (   87.74 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =    4825.94 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      64.20 ms /   188 runs   (    0.34 ms per token,  2928.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     404.10 ms /    29 tokens (   13.93 ms per token,    71.76 tokens per second)\n",
      "llama_print_timings:        eval time =   15296.23 ms /   187 runs   (   81.80 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =   16786.40 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      60.08 ms /   212 runs   (    0.28 ms per token,  3528.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     686.39 ms /    52 tokens (   13.20 ms per token,    75.76 tokens per second)\n",
      "llama_print_timings:        eval time =   17148.76 ms /   211 runs   (   81.27 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =   18809.16 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      51.03 ms /    65 runs   (    0.79 ms per token,  1273.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     871.37 ms /    53 tokens (   16.44 ms per token,    60.82 tokens per second)\n",
      "llama_print_timings:        eval time =    5034.03 ms /    64 runs   (   78.66 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =    6484.57 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      46.43 ms /    61 runs   (    0.76 ms per token,  1313.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     669.26 ms /    45 tokens (   14.87 ms per token,    67.24 tokens per second)\n",
      "llama_print_timings:        eval time =    4773.41 ms /    60 runs   (   79.56 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    5973.64 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     123.98 ms /   160 runs   (    0.77 ms per token,  1290.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     654.30 ms /    33 tokens (   19.83 ms per token,    50.44 tokens per second)\n",
      "llama_print_timings:        eval time =   12581.83 ms /   159 runs   (   79.13 ms per token,    12.64 tokens per second)\n",
      "llama_print_timings:       total time =   14688.12 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid control character at: line 1 column 176 (char 175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     153.58 ms /   186 runs   (    0.83 ms per token,  1211.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.88 ms /    45 tokens (   12.86 ms per token,    77.74 tokens per second)\n",
      "llama_print_timings:        eval time =   14631.08 ms /   185 runs   (   79.09 ms per token,    12.64 tokens per second)\n",
      "llama_print_timings:       total time =   16967.78 ms /   230 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      60.17 ms /    78 runs   (    0.77 ms per token,  1296.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     684.21 ms /    62 tokens (   11.04 ms per token,    90.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6031.53 ms /    77 runs   (   78.33 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =    7421.41 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      28.32 ms /    69 runs   (    0.41 ms per token,  2436.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     663.02 ms /    52 tokens (   12.75 ms per token,    78.43 tokens per second)\n",
      "llama_print_timings:        eval time =    5830.71 ms /    68 runs   (   85.75 ms per token,    11.66 tokens per second)\n",
      "llama_print_timings:       total time =    6854.33 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       8.99 ms /    49 runs   (    0.18 ms per token,  5452.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     665.21 ms /    37 tokens (   17.98 ms per token,    55.62 tokens per second)\n",
      "llama_print_timings:        eval time =    4029.30 ms /    48 runs   (   83.94 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =    4833.63 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.15 ms /    64 runs   (    0.35 ms per token,  2889.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     847.69 ms /    48 tokens (   17.66 ms per token,    56.62 tokens per second)\n",
      "llama_print_timings:        eval time =    4965.97 ms /    63 runs   (   78.82 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =    6159.06 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     153.71 ms /   247 runs   (    0.62 ms per token,  1606.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     949.78 ms /    83 tokens (   11.44 ms per token,    87.39 tokens per second)\n",
      "llama_print_timings:        eval time =   20043.09 ms /   246 runs   (   81.48 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =   23007.00 ms /   329 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 53 (char 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      47.84 ms /    56 runs   (    0.85 ms per token,  1170.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     571.62 ms /    40 tokens (   14.29 ms per token,    69.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4276.59 ms /    55 runs   (   77.76 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =    5390.32 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      66.52 ms /    81 runs   (    0.82 ms per token,  1217.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     924.38 ms /    65 tokens (   14.22 ms per token,    70.32 tokens per second)\n",
      "llama_print_timings:        eval time =    6255.81 ms /    80 runs   (   78.20 ms per token,    12.79 tokens per second)\n",
      "llama_print_timings:       total time =    7937.87 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      28.93 ms /    40 runs   (    0.72 ms per token,  1382.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     396.64 ms /    28 tokens (   14.17 ms per token,    70.59 tokens per second)\n",
      "llama_print_timings:        eval time =    3200.86 ms /    39 runs   (   82.07 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =    3922.43 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      56.18 ms /   125 runs   (    0.45 ms per token,  2224.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     659.46 ms /    37 tokens (   17.82 ms per token,    56.11 tokens per second)\n",
      "llama_print_timings:        eval time =   10310.42 ms /   124 runs   (   83.15 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   11713.68 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 53 (char 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      50.86 ms /   133 runs   (    0.38 ms per token,  2615.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.15 ms /    42 tokens (   13.91 ms per token,    71.90 tokens per second)\n",
      "llama_print_timings:        eval time =   10352.52 ms /   132 runs   (   78.43 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =   11737.80 ms /   174 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      21.45 ms /    60 runs   (    0.36 ms per token,  2796.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     667.83 ms /    44 tokens (   15.18 ms per token,    65.88 tokens per second)\n",
      "llama_print_timings:        eval time =    4599.11 ms /    59 runs   (   77.95 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =    5609.50 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.02 ms /    60 runs   (    0.37 ms per token,  2725.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     665.24 ms /    44 tokens (   15.12 ms per token,    66.14 tokens per second)\n",
      "llama_print_timings:        eval time =    4579.89 ms /    59 runs   (   77.63 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =    5592.52 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      52.69 ms /   142 runs   (    0.37 ms per token,  2694.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     663.34 ms /    42 tokens (   15.79 ms per token,    63.32 tokens per second)\n",
      "llama_print_timings:        eval time =   11264.33 ms /   141 runs   (   79.89 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   12765.77 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      18.86 ms /    48 runs   (    0.39 ms per token,  2545.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     404.25 ms /    32 tokens (   12.63 ms per token,    79.16 tokens per second)\n",
      "llama_print_timings:        eval time =    3655.10 ms /    47 runs   (   77.77 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =    4357.42 ms /    79 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.62 ms /    63 runs   (    0.36 ms per token,  2785.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     661.47 ms /    48 tokens (   13.78 ms per token,    72.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4825.65 ms /    62 runs   (   77.83 ms per token,    12.85 tokens per second)\n",
      "llama_print_timings:       total time =    5863.95 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      17.05 ms /    46 runs   (    0.37 ms per token,  2698.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     402.91 ms /    30 tokens (   13.43 ms per token,    74.46 tokens per second)\n",
      "llama_print_timings:        eval time =    3483.52 ms /    45 runs   (   77.41 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:       total time =    4155.14 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      46.91 ms /   127 runs   (    0.37 ms per token,  2707.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     662.73 ms /    38 tokens (   17.44 ms per token,    57.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9981.68 ms /   126 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =   11411.08 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 53 (char 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      50.66 ms /   139 runs   (    0.36 ms per token,  2743.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     570.83 ms /    33 tokens (   17.30 ms per token,    57.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10983.89 ms /   138 runs   (   79.59 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =   12381.86 ms /   171 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.44 ms /    62 runs   (    0.36 ms per token,  2762.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     666.45 ms /    46 tokens (   14.49 ms per token,    69.02 tokens per second)\n",
      "llama_print_timings:        eval time =    4736.46 ms /    61 runs   (   77.65 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =    5763.65 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      22.84 ms /    65 runs   (    0.35 ms per token,  2846.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     668.37 ms /    49 tokens (   13.64 ms per token,    73.31 tokens per second)\n",
      "llama_print_timings:        eval time =    4997.98 ms /    64 runs   (   78.09 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =    6037.98 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      85.25 ms /   231 runs   (    0.37 ms per token,  2709.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     665.94 ms /    47 tokens (   14.17 ms per token,    70.58 tokens per second)\n",
      "llama_print_timings:        eval time =   18555.33 ms /   230 runs   (   80.68 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =   20731.01 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      70.41 ms /   179 runs   (    0.39 ms per token,  2542.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.77 ms /    44 tokens (   15.81 ms per token,    63.24 tokens per second)\n",
      "llama_print_timings:        eval time =   14270.53 ms /   178 runs   (   80.17 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =   16137.10 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      20.13 ms /    58 runs   (    0.35 ms per token,  2881.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     673.93 ms /    44 tokens (   15.32 ms per token,    65.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4434.76 ms /    57 runs   (   77.80 ms per token,    12.85 tokens per second)\n",
      "llama_print_timings:       total time =    5448.23 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 160 (char 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      55.28 ms /   148 runs   (    0.37 ms per token,  2677.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     592.15 ms /    60 tokens (    9.87 ms per token,   101.33 tokens per second)\n",
      "llama_print_timings:        eval time =   11534.60 ms /   147 runs   (   78.47 ms per token,    12.74 tokens per second)\n",
      "llama_print_timings:       total time =   13024.22 ms /   207 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      18.18 ms /    49 runs   (    0.37 ms per token,  2695.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     661.90 ms /    37 tokens (   17.89 ms per token,    55.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3721.64 ms /    48 runs   (   77.53 ms per token,    12.90 tokens per second)\n",
      "llama_print_timings:       total time =    4670.16 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      64.34 ms /   169 runs   (    0.38 ms per token,  2626.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     660.75 ms /    36 tokens (   18.35 ms per token,    54.48 tokens per second)\n",
      "llama_print_timings:        eval time =   13211.79 ms /   168 runs   (   78.64 ms per token,    12.72 tokens per second)\n",
      "llama_print_timings:       total time =   14908.24 ms /   204 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      25.14 ms /    69 runs   (    0.36 ms per token,  2744.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     670.10 ms /    52 tokens (   12.89 ms per token,    77.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5280.74 ms /    68 runs   (   77.66 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =    6356.60 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      26.85 ms /    74 runs   (    0.36 ms per token,  2755.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     682.11 ms /    58 tokens (   11.76 ms per token,    85.03 tokens per second)\n",
      "llama_print_timings:        eval time =    5703.19 ms /    73 runs   (   78.13 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:       total time =    6811.74 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      66.74 ms /   175 runs   (    0.38 ms per token,  2622.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     670.21 ms /    48 tokens (   13.96 ms per token,    71.62 tokens per second)\n",
      "llama_print_timings:        eval time =   13588.21 ms /   174 runs   (   78.09 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =   15355.04 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      14.83 ms /    43 runs   (    0.34 ms per token,  2899.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     421.56 ms /    27 tokens (   15.61 ms per token,    64.05 tokens per second)\n",
      "llama_print_timings:        eval time =    3263.72 ms /    42 runs   (   77.71 ms per token,    12.87 tokens per second)\n",
      "llama_print_timings:       total time =    3929.14 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      55.30 ms /   144 runs   (    0.38 ms per token,  2603.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     403.53 ms /    27 tokens (   14.95 ms per token,    66.91 tokens per second)\n",
      "llama_print_timings:        eval time =   11117.78 ms /   143 runs   (   77.75 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   12400.67 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 153)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      16.31 ms /    46 runs   (    0.35 ms per token,  2820.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     318.10 ms /    31 tokens (   10.26 ms per token,    97.45 tokens per second)\n",
      "llama_print_timings:        eval time =    3530.55 ms /    45 runs   (   78.46 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =    4116.20 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      51.98 ms /   214 runs   (    0.24 ms per token,  4116.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     654.19 ms /    36 tokens (   18.17 ms per token,    55.03 tokens per second)\n",
      "llama_print_timings:        eval time =   18889.28 ms /   213 runs   (   88.68 ms per token,    11.28 tokens per second)\n",
      "llama_print_timings:       total time =   20430.54 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substring not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =       9.63 ms /    35 runs   (    0.28 ms per token,  3633.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     311.44 ms /    20 tokens (   15.57 ms per token,    64.22 tokens per second)\n",
      "llama_print_timings:        eval time =    2871.13 ms /    34 runs   (   84.45 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =    3349.46 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      20.32 ms /    58 runs   (    0.35 ms per token,  2855.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     659.38 ms /    43 tokens (   15.33 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4451.87 ms /    57 runs   (   78.10 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:       total time =    5432.69 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      21.16 ms /    74 runs   (    0.29 ms per token,  3496.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     672.71 ms /    59 tokens (   11.40 ms per token,    87.70 tokens per second)\n",
      "llama_print_timings:        eval time =    5845.45 ms /    73 runs   (   80.07 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =    6847.98 ms /   132 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      15.89 ms /    86 runs   (    0.18 ms per token,  5412.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     413.26 ms /    13 tokens (   31.79 ms per token,    31.46 tokens per second)\n",
      "llama_print_timings:        eval time =    7196.23 ms /    85 runs   (   84.66 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:       total time =    7848.38 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =     104.56 ms /   286 runs   (    0.37 ms per token,  2735.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     940.82 ms /    76 tokens (   12.38 ms per token,    80.78 tokens per second)\n",
      "llama_print_timings:        eval time =   22818.91 ms /   285 runs   (   80.07 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =   25534.93 ms /   361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      23.79 ms /    66 runs   (    0.36 ms per token,  2774.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     399.48 ms /    18 tokens (   22.19 ms per token,    45.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5119.11 ms /    65 runs   (   78.76 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:       total time =    5877.10 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     524.70 ms\n",
      "llama_print_timings:      sample time =      30.94 ms /    86 runs   (    0.36 ms per token,  2779.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     936.91 ms /    71 tokens (   13.20 ms per token,    75.78 tokens per second)\n",
      "llama_print_timings:        eval time =    6685.19 ms /    85 runs   (   78.65 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =    8111.07 ms /   156 tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract triples for each context in eval.json and create pred.json\n",
    "def generate_pred_json(eval_file_path, pred_file_path):\n",
    "    # Load the evaluation data from eval.json\n",
    "    with open(eval_file_path, 'r') as file:\n",
    "        eval_data = json.load(file)\n",
    "    \n",
    "    # Initialize a list to hold the modified data with extracted triples\n",
    "    modified_data = []\n",
    "    \n",
    "    # Iterate over each item in the evaluation data\n",
    "    for item in eval_data:\n",
    "        context = item['context']\n",
    "        # Prepare the command with the context\n",
    "        command = f\"Can you please give triple for \\\"{context}\\\"\"\n",
    "        # Use the process_command function to predict the extracted triples\n",
    "        extracted_triplets = process_command(command)\n",
    "        # Append the extracted triples to the item under the 'triples' key\n",
    "        item['triples'] = extracted_triplets\n",
    "        # Append the modified item to the modified_data list\n",
    "        modified_data.append(item)\n",
    "    \n",
    "    # Write the modified data with extracted triples to pred.json\n",
    "    with open(pred_file_path, 'w') as file:\n",
    "        json.dump(modified_data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "eval_file_path = '/Users/ananyahooda/Desktop/Text2kg/conll04_eval.json' # Replace with the actual path to your eval.json file\n",
    "pred_file_path = '/Users/ananyahooda/Desktop/Text2kg/pred_conll04.json' # The output file path\n",
    "generate_pred_json(eval_file_path, pred_file_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25c6fead536226664bc243aad9801a1612911a7cb4d3a0685925e49c59352eff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 64-bit ('aiml1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
