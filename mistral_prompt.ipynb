{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/ccc66ybs0m59z0yyhly4_jp80000gn/T/ipykernel_4922/661250872.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3877.58 MiB, ( 3877.64 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3877.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 4135.45 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, ( 4299.47 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf\",  \n",
    "n_ctx=2048,\n",
    "n_gpu_layers=-1,\n",
    "n_batch=512,\n",
    "callback_manager=callback_manager,\n",
    "verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Please extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(question, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The available action to Assistant is:\n",
    "# - \"extract_text_triplets\": Useful for when Assistant is asked to extract triplets from a given text.\n",
    "#   - To use the extract_triplets tool, Assistant should respond like so:\n",
    "#     {{\"action\": \"extract_text_triplets\", \"action_input\": \"Your text here\"}}\n",
    "\n",
    "# Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "# User: Hey how are you today?\n",
    "# Assistant: I'm good thanks, how are you?\n",
    "# User: Can you extract all the triplets from this text: \"Gràcia is a district of the city of Barcelona, Spain.\"\n",
    "# Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"Gràcia is a district of the city of Barcelona, Spain.\"}}\n",
    "# User: Also give triples for \"obama was US president\"\n",
    "# Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"obama was US president\"}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''<s>[INST] <<SYS>>\n",
    "Assistant is an expert JSON builder designed to assist with a tasks of Triple extraction from text.\n",
    "\n",
    "Assistant is able to extract triples from the text.\n",
    "\n",
    "Only type on relation available are 5 realtions: {{'employer', 'headquarters location', 'killed by', 'location', 'residence'}}\n",
    "\n",
    "Only type of entities are: {{'People', 'Organisation', 'Location', 'Other'}}\n",
    "\n",
    "Assistant returns the response as the list of dictionaries only always and dictionaries consisting of keys called \"head\", \"type\" and \"tail\" in ever dictionary in the list\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: I'm good thanks, how are you?\n",
    "User: Can you extract all the triplets from this text: John Wilkes Booth , who assassinated President Lincoln , was an actor .\n",
    "Assistant:  [{{\"head\": \"President Lincoln\", \"type\": \"killed by\", \"tail\": \"John Wilkes Booth\"}}]\n",
    "User: Also give triples for text: \"Marie Magdefrau Ferraro , 50 , of Bethany , Conn. , was shot to death Thursday when two bandits armed with assault rifles emerged from nearby bushes and began firing at a van carrying a Connecticut Audubon Society wildlife wild tour group .\n",
    "Assistant: [{{\"head\": \"Marie Magdefrau Ferraro\", \"type\": \"residence\", \"tail\": \"Bethany\"}}, {{\"head\": \"Marie Magdefrau Ferraro\", \"type\": \"residence\", \"tail\": \"Conn.\" }},{{\"head\": \"Bethany\", \"type\": \"location\", \"tail\": \"Conn.\"}}] \n",
    "User: Thanks, Bye!\n",
    "Assistant: See you later, the Chat is closed.\n",
    "<</SYS>>\n",
    "\n",
    "{0}[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \"Please extract triples for: In the field of mechanics , Wang Ziqiang at the Institute of Mechanics has made considerable headway in the area of elastoplastic crack mechanics .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_command1(command):\n",
    "    # Put user command into prompt\n",
    "    prompt = prompt_template.format(\"User: \" + command)\n",
    "    # Send command to the model\n",
    "    output = llm(prompt, max_tokens=2000, stop=[\"User:\"])\n",
    "    response = (output['choices'][0]['text']).strip()\n",
    "    response = ast.literal_eval(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      25.50 ms /    85 runs   (    0.30 ms per token,  3332.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4083.89 ms /   355 tokens (   11.50 ms per token,    86.93 tokens per second)\n",
      "llama_print_timings:        eval time =    6740.42 ms /    84 runs   (   80.24 ms per token,    12.46 tokens per second)\n",
      "llama_print_timings:       total time =   11306.71 ms /   439 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'Wang Ziqiang',\n",
       "  'type': 'headquarters location',\n",
       "  'tail': 'Institute of Mechanics'},\n",
       " {'head': 'Wang Ziqiang',\n",
       "  'type': 'employer',\n",
       "  'tail': 'Institute of Mechanics'},\n",
       " {'head': 'Institute of Mechanics',\n",
       "  'type': 'location',\n",
       "  'tail': 'field of mechanics'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command1(\"Please extract triples for: In the field of mechanics , Wang Ziqiang at the Institute of Mechanics has made considerable headway in the area of elastoplastic crack mechanics .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      10.56 ms /    55 runs   (    0.19 ms per token,  5209.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     422.58 ms /    26 tokens (   16.25 ms per token,    61.53 tokens per second)\n",
      "llama_print_timings:        eval time =    4617.62 ms /    54 runs   (   85.51 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =    5232.67 ms /    80 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'President Lincoln',\n",
       "  'type': 'killed by',\n",
       "  'tail': 'John Wilkes Booth'},\n",
       " {'head': 'John Wilkes Booth', 'type': 'profession', 'tail': 'actor'}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command1(\"Extract triples for: John Wilkes Booth , who assassinated President Lincoln , was an actor .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      44.57 ms /   160 runs   (    0.28 ms per token,  3589.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     891.24 ms /    44 tokens (   20.26 ms per token,    49.37 tokens per second)\n",
      "llama_print_timings:        eval time =   12858.51 ms /   159 runs   (   80.87 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   14693.93 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: unexpected indent (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =     109.99 ms /   363 runs   (    0.30 ms per token,  3300.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.50 ms /    37 tokens (   15.61 ms per token,    64.07 tokens per second)\n",
      "llama_print_timings:        eval time =   29238.47 ms /   362 runs   (   80.77 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =   32182.37 ms /   399 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: EOL while scanning string literal (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      45.94 ms /   159 runs   (    0.29 ms per token,  3461.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     315.29 ms /    23 tokens (   13.71 ms per token,    72.95 tokens per second)\n",
      "llama_print_timings:        eval time =   12582.45 ms /   158 runs   (   79.64 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =   13864.38 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      12.21 ms /    44 runs   (    0.28 ms per token,  3602.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     321.27 ms /    30 tokens (   10.71 ms per token,    93.38 tokens per second)\n",
      "llama_print_timings:        eval time =    3415.64 ms /    43 runs   (   79.43 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:       total time =    3989.26 ms /    73 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      43.94 ms /   146 runs   (    0.30 ms per token,  3322.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     588.65 ms /    49 tokens (   12.01 ms per token,    83.24 tokens per second)\n",
      "llama_print_timings:        eval time =   11567.20 ms /   145 runs   (   79.77 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =   13052.43 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      26.32 ms /    91 runs   (    0.29 ms per token,  3456.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     312.26 ms /    32 tokens (    9.76 ms per token,   102.48 tokens per second)\n",
      "llama_print_timings:        eval time =    7161.49 ms /    90 runs   (   79.57 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    8018.66 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      16.23 ms /    58 runs   (    0.28 ms per token,  3572.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.38 ms /    42 tokens (   13.84 ms per token,    72.24 tokens per second)\n",
      "llama_print_timings:        eval time =    4524.97 ms /    57 runs   (   79.39 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =    5435.76 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =       6.92 ms /    28 runs   (    0.25 ms per token,  4046.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.58 ms /    18 tokens (   17.20 ms per token,    58.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2151.15 ms /    27 runs   (   79.67 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =    2602.07 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      25.98 ms /    89 runs   (    0.29 ms per token,  3426.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.77 ms /    48 tokens (   12.22 ms per token,    81.80 tokens per second)\n",
      "llama_print_timings:        eval time =    7022.89 ms /    88 runs   (   79.81 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    8136.90 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      17.54 ms /    64 runs   (    0.27 ms per token,  3648.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.08 ms /    43 tokens (   13.44 ms per token,    74.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5008.50 ms /    63 runs   (   79.50 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    5947.91 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =       6.90 ms /    26 runs   (    0.27 ms per token,  3765.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     308.34 ms /    17 tokens (   18.14 ms per token,    55.13 tokens per second)\n",
      "llama_print_timings:        eval time =    1988.96 ms /    25 runs   (   79.56 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    2442.45 ms /    42 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      29.79 ms /   101 runs   (    0.29 ms per token,  3390.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     308.40 ms /    32 tokens (    9.64 ms per token,   103.76 tokens per second)\n",
      "llama_print_timings:        eval time =    7948.99 ms /   100 runs   (   79.49 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    8859.71 ms /   132 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =       7.85 ms /    28 runs   (    0.28 ms per token,  3568.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.30 ms /    42 tokens (   13.79 ms per token,    72.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2148.80 ms /    27 runs   (   79.59 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    2876.31 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      47.01 ms /   156 runs   (    0.30 ms per token,  3318.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.42 ms /    54 tokens (   10.95 ms per token,    91.31 tokens per second)\n",
      "llama_print_timings:        eval time =   12375.35 ms /   155 runs   (   79.84 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   13953.16 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /    30 runs   (    0.26 ms per token,  3841.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     317.34 ms /    26 tokens (   12.21 ms per token,    81.93 tokens per second)\n",
      "llama_print_timings:        eval time =    2305.16 ms /    29 runs   (   79.49 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    2782.50 ms /    55 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      18.98 ms /    66 runs   (    0.29 ms per token,  3477.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.33 ms /    41 tokens (   14.11 ms per token,    70.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5176.51 ms /    65 runs   (   79.64 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =    6133.82 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      38.80 ms /   131 runs   (    0.30 ms per token,  3376.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     600.43 ms /    61 tokens (    9.84 ms per token,   101.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10387.07 ms /   130 runs   (   79.90 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   11786.61 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      45.21 ms /   155 runs   (    0.29 ms per token,  3428.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.28 ms /    32 tokens (    9.70 ms per token,   103.13 tokens per second)\n",
      "llama_print_timings:        eval time =   12370.47 ms /   154 runs   (   80.33 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =   13605.59 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      38.44 ms /   132 runs   (    0.29 ms per token,  3434.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     594.09 ms /    58 tokens (   10.24 ms per token,    97.63 tokens per second)\n",
      "llama_print_timings:        eval time =   10464.02 ms /   131 runs   (   79.88 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   11868.17 ms /   189 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: EOL while scanning string literal (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =       7.35 ms /    27 runs   (    0.27 ms per token,  3671.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.07 ms /    47 tokens (   12.45 ms per token,    80.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2060.30 ms /    26 runs   (   79.24 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    2800.84 ms /    73 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      17.06 ms /    59 runs   (    0.29 ms per token,  3457.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.59 ms /    53 tokens (   11.16 ms per token,    89.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4612.97 ms /    58 runs   (   79.53 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    5543.69 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      17.42 ms /    58 runs   (    0.30 ms per token,  3329.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.93 ms /    43 tokens (   13.51 ms per token,    74.02 tokens per second)\n",
      "llama_print_timings:        eval time =    4532.55 ms /    57 runs   (   79.52 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    5456.26 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: EOL while scanning string literal (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      23.76 ms /    81 runs   (    0.29 ms per token,  3408.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.52 ms /    36 tokens (   16.04 ms per token,    62.34 tokens per second)\n",
      "llama_print_timings:        eval time =    6351.46 ms /    80 runs   (   79.39 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =    7410.93 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      31.20 ms /   105 runs   (    0.30 ms per token,  3365.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.50 ms /    53 tokens (   11.14 ms per token,    89.75 tokens per second)\n",
      "llama_print_timings:        eval time =    8287.08 ms /   104 runs   (   79.68 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =    9516.02 ms /   157 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: EOL while scanning string literal (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      35.85 ms /   123 runs   (    0.29 ms per token,  3430.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     594.94 ms /    59 tokens (   10.08 ms per token,    99.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9766.65 ms /   122 runs   (   80.05 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =   11115.38 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      31.57 ms /   106 runs   (    0.30 ms per token,  3358.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.61 ms /    39 tokens (   14.81 ms per token,    67.52 tokens per second)\n",
      "llama_print_timings:        eval time =    8348.90 ms /   105 runs   (   79.51 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    9559.16 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =     103.19 ms /   340 runs   (    0.30 ms per token,  3294.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.98 ms /    47 tokens (   12.43 ms per token,    80.48 tokens per second)\n",
      "llama_print_timings:        eval time =   27257.11 ms /   339 runs   (   80.40 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   30080.00 ms /   386 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: EOL while scanning string literal (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      23.43 ms /    79 runs   (    0.30 ms per token,  3372.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     597.36 ms /    61 tokens (    9.79 ms per token,   102.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6225.48 ms /    78 runs   (   79.81 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    7299.40 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =       7.93 ms /    31 runs   (    0.26 ms per token,  3909.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     313.76 ms /    24 tokens (   13.07 ms per token,    76.49 tokens per second)\n",
      "llama_print_timings:        eval time =    2380.61 ms /    30 runs   (   79.35 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =    2862.88 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      53.79 ms /   184 runs   (    0.29 ms per token,  3420.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.18 ms /    52 tokens (   11.35 ms per token,    88.11 tokens per second)\n",
      "llama_print_timings:        eval time =   14619.71 ms /   183 runs   (   79.89 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   16360.31 ms /   235 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      10.02 ms /    37 runs   (    0.27 ms per token,  3691.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     588.73 ms /    50 tokens (   11.77 ms per token,    84.93 tokens per second)\n",
      "llama_print_timings:        eval time =    2855.78 ms /    36 runs   (   79.33 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =    3669.05 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      18.28 ms /    62 runs   (    0.29 ms per token,  3392.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.23 ms /    49 tokens (   11.96 ms per token,    83.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4847.70 ms /    61 runs   (   79.47 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    5788.57 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      21.58 ms /    77 runs   (    0.28 ms per token,  3567.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.51 ms /    49 tokens (   11.97 ms per token,    83.55 tokens per second)\n",
      "llama_print_timings:        eval time =    6044.99 ms /    76 runs   (   79.54 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    7074.46 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      27.74 ms /    94 runs   (    0.30 ms per token,  3388.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     573.02 ms /    33 tokens (   17.36 ms per token,    57.59 tokens per second)\n",
      "llama_print_timings:        eval time =    7385.01 ms /    93 runs   (   79.41 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:       total time =    8526.81 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      54.07 ms /   184 runs   (    0.29 ms per token,  3403.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.09 ms /    54 tokens (   10.95 ms per token,    91.36 tokens per second)\n",
      "llama_print_timings:        eval time =   14816.95 ms /   183 runs   (   80.97 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =   16506.21 ms /   237 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      21.85 ms /    75 runs   (    0.29 ms per token,  3432.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.00 ms /    49 tokens (   11.98 ms per token,    83.48 tokens per second)\n",
      "llama_print_timings:        eval time =    5885.84 ms /    74 runs   (   79.54 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    6912.50 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      23.76 ms /    82 runs   (    0.29 ms per token,  3451.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     321.15 ms /    31 tokens (   10.36 ms per token,    96.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6427.40 ms /    81 runs   (   79.35 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =    7223.03 ms /   112 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      41.60 ms /   140 runs   (    0.30 ms per token,  3365.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     599.89 ms /    62 tokens (    9.68 ms per token,   103.35 tokens per second)\n",
      "llama_print_timings:        eval time =   11092.90 ms /   139 runs   (   79.81 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =   12550.51 ms /   201 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: closing parenthesis ')' does not match opening parenthesis '[' (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      43.42 ms /   149 runs   (    0.29 ms per token,  3431.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.22 ms /    45 tokens (   12.96 ms per token,    77.16 tokens per second)\n",
      "llama_print_timings:        eval time =   11826.45 ms /   148 runs   (   79.91 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =   13320.35 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      60.64 ms /   207 runs   (    0.29 ms per token,  3413.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.84 ms /    45 tokens (   12.93 ms per token,    77.34 tokens per second)\n",
      "llama_print_timings:        eval time =   16550.77 ms /   206 runs   (   80.34 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =   18398.21 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      27.61 ms /    85 runs   (    0.32 ms per token,  3079.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.61 ms /    44 tokens (   13.60 ms per token,    73.50 tokens per second)\n",
      "llama_print_timings:        eval time =    7462.67 ms /    84 runs   (   88.84 ms per token,    11.26 tokens per second)\n",
      "llama_print_timings:       total time =    8522.42 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: EOL while scanning string literal (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =     101.06 ms /   173 runs   (    0.58 ms per token,  1711.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     862.18 ms /    73 tokens (   11.81 ms per token,    84.67 tokens per second)\n",
      "llama_print_timings:        eval time =   14276.64 ms /   172 runs   (   83.00 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   16721.10 ms /   245 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =     137.63 ms /   227 runs   (    0.61 ms per token,  1649.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     880.53 ms /    84 tokens (   10.48 ms per token,    95.40 tokens per second)\n",
      "llama_print_timings:        eval time =   18483.09 ms /   226 runs   (   81.78 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =   21485.20 ms /   310 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      41.73 ms /    80 runs   (    0.52 ms per token,  1916.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.19 ms /    47 tokens (   12.56 ms per token,    79.64 tokens per second)\n",
      "llama_print_timings:        eval time =    6568.53 ms /    79 runs   (   83.15 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =    7799.60 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      34.53 ms /    78 runs   (    0.44 ms per token,  2259.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     329.76 ms /    28 tokens (   11.78 ms per token,    84.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6623.08 ms /    77 runs   (   86.01 ms per token,    11.63 tokens per second)\n",
      "llama_print_timings:       total time =    7497.27 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      74.06 ms /   126 runs   (    0.59 ms per token,  1701.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.25 ms /    49 tokens (   12.03 ms per token,    83.16 tokens per second)\n",
      "llama_print_timings:        eval time =   10067.23 ms /   125 runs   (   80.54 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =   11799.73 ms /   174 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      22.83 ms /    50 runs   (    0.46 ms per token,  2189.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     593.03 ms /    48 tokens (   12.35 ms per token,    80.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4078.67 ms /    49 runs   (   83.24 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =    5028.25 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      47.44 ms /    91 runs   (    0.52 ms per token,  1918.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     878.08 ms /    73 tokens (   12.03 ms per token,    83.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7619.35 ms /    90 runs   (   84.66 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:       total time =    9193.71 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      58.90 ms /   105 runs   (    0.56 ms per token,  1782.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     575.00 ms /    38 tokens (   15.13 ms per token,    66.09 tokens per second)\n",
      "llama_print_timings:        eval time =    8418.49 ms /   104 runs   (   80.95 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    9933.01 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =     110.04 ms /   283 runs   (    0.39 ms per token,  2571.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.03 ms /    17 tokens (   18.18 ms per token,    55.01 tokens per second)\n",
      "llama_print_timings:        eval time =   24295.98 ms /   282 runs   (   86.16 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:       total time =   26506.05 ms /   299 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =       9.02 ms /    41 runs   (    0.22 ms per token,  4546.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.25 ms /    35 tokens (   16.55 ms per token,    60.42 tokens per second)\n",
      "llama_print_timings:        eval time =    3274.79 ms /    40 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =    4046.65 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      42.84 ms /    89 runs   (    0.48 ms per token,  2077.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.05 ms /    38 tokens (   15.24 ms per token,    65.62 tokens per second)\n",
      "llama_print_timings:        eval time =    7460.11 ms /    88 runs   (   84.77 ms per token,    11.80 tokens per second)\n",
      "llama_print_timings:       total time =    8748.38 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      55.12 ms /   112 runs   (    0.49 ms per token,  2031.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.02 ms /    42 tokens (   14.02 ms per token,    71.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9164.86 ms /   111 runs   (   82.57 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   10626.23 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      35.69 ms /    72 runs   (    0.50 ms per token,  2017.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     856.47 ms /    67 tokens (   12.78 ms per token,    78.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6043.86 ms /    71 runs   (   85.12 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =    7410.03 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      44.84 ms /    92 runs   (    0.49 ms per token,  2051.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     576.95 ms /    35 tokens (   16.48 ms per token,    60.66 tokens per second)\n",
      "llama_print_timings:        eval time =    8246.99 ms /    91 runs   (   90.63 ms per token,    11.03 tokens per second)\n",
      "llama_print_timings:       total time =    9458.75 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =     117.53 ms /   240 runs   (    0.49 ms per token,  2042.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1459.12 ms /   142 tokens (   10.28 ms per token,    97.32 tokens per second)\n",
      "llama_print_timings:        eval time =   20222.22 ms /   239 runs   (   84.61 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =   23536.45 ms /   381 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      22.38 ms /    47 runs   (    0.48 ms per token,  2099.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     320.54 ms /    20 tokens (   16.03 ms per token,    62.39 tokens per second)\n",
      "llama_print_timings:        eval time =    3791.43 ms /    46 runs   (   82.42 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    4437.67 ms /    66 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      44.86 ms /   108 runs   (    0.42 ms per token,  2407.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     871.73 ms /    67 tokens (   13.01 ms per token,    76.86 tokens per second)\n",
      "llama_print_timings:        eval time =    8910.46 ms /   107 runs   (   83.28 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =   10492.31 ms /   174 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      48.31 ms /   104 runs   (    0.46 ms per token,  2152.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.27 ms /    39 tokens (   15.01 ms per token,    66.64 tokens per second)\n",
      "llama_print_timings:        eval time =    8825.88 ms /   103 runs   (   85.69 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =   10156.89 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      24.91 ms /    49 runs   (    0.51 ms per token,  1966.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     592.18 ms /    45 tokens (   13.16 ms per token,    75.99 tokens per second)\n",
      "llama_print_timings:        eval time =    4001.07 ms /    48 runs   (   83.36 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =    4939.45 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      15.84 ms /    31 runs   (    0.51 ms per token,  1957.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     320.68 ms /    16 tokens (   20.04 ms per token,    49.89 tokens per second)\n",
      "llama_print_timings:        eval time =    2630.56 ms /    30 runs   (   87.69 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =    3174.15 ms /    46 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =      39.97 ms /    96 runs   (    0.42 ms per token,  2402.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.64 ms /    38 tokens (   15.39 ms per token,    65.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7766.43 ms /    95 runs   (   81.75 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    8986.35 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8692.62 ms\n",
      "llama_print_timings:      sample time =     173.46 ms /   375 runs   (    0.46 ms per token,  2161.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     619.15 ms /    62 tokens (    9.99 ms per token,   100.14 tokens per second)\n",
      "llama_print_timings:        eval time =   34110.77 ms /   374 runs   (   91.21 ms per token,    10.96 tokens per second)\n",
      "llama_print_timings:       total time =   37534.15 ms /   436 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught a SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract triples for each context in eval.json and create pred.json\n",
    "def generate_pred_json(eval_file_path, pred_file_path):\n",
    "    # Load the evaluation data from eval.json\n",
    "    with open(eval_file_path, 'r') as file:\n",
    "        eval_data = json.load(file)\n",
    "    \n",
    "    # Initialize a list to hold the modified data with extracted triples\n",
    "    modified_data = []\n",
    "    \n",
    "    # Iterate over each item in the evaluation data\n",
    "    for item in eval_data:\n",
    "        context = item['context']\n",
    "        # Prepare the command with the context\n",
    "        command = f\"Can you extract all the triplets from this text: {context}\"\n",
    "        # Use the process_command function to predict the extracted triples\n",
    "        try:\n",
    "            extracted_triplets = process_command1(command)\n",
    "            item['triples'] = extracted_triplets\n",
    "        # Append the modified item to the modified_data list\n",
    "            modified_data.append(item)\n",
    "        except SyntaxError as e:\n",
    "            print(f\"Caught a SyntaxError: {e}\")\n",
    "        # Append the extracted triples to the item under the 'triples' key\n",
    "       \n",
    "    \n",
    "    # Write the modified data with extracted triples to pred.json\n",
    "    with open(pred_file_path, 'w') as file:\n",
    "        json.dump(modified_data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "eval_file_path = '/Users/ananyahooda/Desktop/final/data/extracted_data/conll04/conll04_small.json' # Replace with the actual path to your eval.json file\n",
    "pred_file_path = '/Users/ananyahooda/Desktop/final/small_pred_conll04.json' # The output file path\n",
    "generate_pred_json(eval_file_path, pred_file_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4fa2ca3c54d5fbdec4fbc1ede1009e749a4bbc10aad76919c0852744120220"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
