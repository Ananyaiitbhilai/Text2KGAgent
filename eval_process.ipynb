{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the golden truth JSON file: 288\n",
      "Length of the prediction JSON file: 288\n",
      "Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open('test_triples_conll04.json', 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open('pred_conll04.json', 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Determine the length of both JSON files\n",
    "length_data1 = len(data1)\n",
    "length_data2 = len(data2)\n",
    "\n",
    "# Print the lengths\n",
    "print(f\"Length of the golden truth JSON file: {length_data1}\")\n",
    "print(f\"Length of the prediction JSON file: {length_data2}\")\n",
    "\n",
    "# Convert the lists to dictionaries indexed by the 'id' attribute\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "data2_dict = {item['id']: item for item in data2}\n",
    "\n",
    "# Find the common IDs\n",
    "common_ids = set(data1_dict.keys()) & set(data2_dict.keys())\n",
    "\n",
    "# Extract the common data points\n",
    "common_data1 = [data1_dict[id] for id in common_ids]\n",
    "common_data2 = [data2_dict[id] for id in common_ids]\n",
    "\n",
    "# Save the common data points to new JSON files\n",
    "with open('Finaltest.json', 'w') as file:\n",
    "    json.dump(common_data1, file, indent=4)\n",
    "\n",
    "with open('Finalpred.json', 'w') as file:\n",
    "    json.dump(common_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the new files have been created\n",
    "print(f\"Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with 'triples' as a string: 26\n",
      "String 1:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"There was no mention of the ` iron triangle ` of members of Congress, the news media and special interest groups who, in a speech to political appointees in Washington on Dec. 13, Reagan claimed had prevented his administration from balancing the federal budget.\"}\n",
      "String 2:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Aguadilla is a city in Puerto Rico.\"}\n",
      "\n",
      "Or if you meant to provide the triplet for the statement \"Aguadilla is in Puerto Rico\", then the response would be:\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"Aguadilla is in Puerto Rico.\"}\n",
      "String 3:  {\"action\": \"extract_text_triplets\", \"action\\_input\": \"The old bugaboo is where this baby is going to hit,\" \" said John Jamison, a National Weather Service meteorologist in Galveston.\"}\n",
      "\n",
      "This will extract subject-verb-object triplets from the given text, such as [\"John Jamison\", \"said\", \"was going to hit\"] or [\"the old bugaboo\", \"is\", \"where this baby is going to hit\"].\n",
      "String 4:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"By year 's end , 6 million acres had burned in the West and Alaska , making 1988 the worst fire season in 30 years , and , in terms of firefighting resources committed , the most expensive in U.S. history , Sacher said.\"}\n",
      "\n",
      "Possible extracted triplets:\n",
      "\n",
      "1. (\"Year's end\", \"event\", \"[6 million acres had burned]\")\n",
      "2. (\"West and Alaska\", \"location\", \"[6 million acres had burned]\")\n",
      "3. (\"1988\", \"year\", [\"worse fire season in 30 years\"])\n",
      "4. (\"30 years\", \"duration\", [\"worse fire season in 30 years\"])\n",
      "5. (\"most expensive\", \"adjective\", [\"in U.S. history\"])\n",
      "6. (\"firefighting resources committed\", \"resources\", [\"most expensive in U.S. history\"])\n",
      "7. (\"Sacher said\", \"speaker\", [\". .\"])\n",
      "String 5:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"That original one was knocked down for a reason by the tanks , and I presume this one was knocked down for the same reason , ' Santa Monica artist Tom Van Sant said Monday after the 23-foot-tall statue was found crushed and broken in pieces.\"}\n",
      "\n",
      "Here are the potential triplets from the given text:\n",
      "\n",
      "1. [Santa Monica artist Tom Van Sant, said, Monday]\n",
      "2. [Santa Monica artist Tom Van Sant, presumed, same reason]\n",
      "3. [Original statue, was knocked down, reason]\n",
      "4. [New statue, was found, crushed and broken in pieces]\n",
      "5. [Tom Van Sant, spoke, Monday]\n",
      "String 6:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Bush, who acknowledged that Sununu was not quiet and retiring also had words for Virginia's Baliles, who was stepping down after a year as chairman of the association.\")\n",
      "}\n",
      "\n",
      "Triplets: [[\"Bush\", \"had words for\", \"Baliles\"], [\"Baliles\", \"was stepping down\", \"after a year\"], [\"Sununu\", \"was not quiet and retiring\"]]\n",
      "String 7:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"In northeastern Oregon, at least five major fires and several smaller ones burned across 60, 000 acres, forcing the evacuation of some rural homes and threatening the watershed for the city of La Grande, authorities said.\"}\n",
      "String 8:  {\"action\": \"extract_text_triplets\", \"action_input\": \"But Jack Frazier, Rotary Club president, said volunteers picked up the ducks and all but four or five were accounted for.}\n",
      "\n",
      "Possible extracts: [\n",
      "{\"subject\": \"But\", \"predicate\": \"said\", \"object\": \"Jack Frazier\"},\n",
      "{\"subject\": \"Jack Frazier\", \"predicate\": \"was\", \"object\": \"Rotary Club president\"},\n",
      "{\"subject\": \"volunteers\", \"predicate\": \"picked\", \"object\": \"up the ducks\"},\n",
      "{\"subject\": \"all but four or five\", \"predicate\": \"were\", \"object\": \"accounted for.\"}]\n",
      "String 9:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Of 40 million party line calls logged by New England Telephone over a 2-year period, at least 10 percent were made by those who dial services like XBT Telecom's 'Talkabout' teen line, a group conversation designed for people under age 16, said John Johnson, a spokesman for the telephone company.}\n",
      "\n",
      "Possible triplets from the text:\n",
      "\n",
      "1. [\"New England Telephone\", \"logged\", \"40 million party line calls\"]\n",
      "2. [\"Over a 2-year period\", \"at least\", \"10 percent\"]\n",
      "3. [\"People who dial services like XBT Telecom's 'Talkabout'\", \"made\", \"party line calls\"]\n",
      "4. [\"XBT Telecom's 'Talkabout'\", \"is\", \"a group conversation\"]\n",
      "5. [\"People under age 16\", \"designed for\", \"XBT Telephone's 'Talkabout'\"]\n",
      "6. [\"John Johnson\", \"said\", \"spokesman for the telephone company\"]\n",
      "String 10:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Senate Judiciary Committee Chairman Joseph R. Biden, D-Del., said he plans to hold four hearings on anti-flag burning proposals.}\n",
      "\n",
      "Triplets:\n",
      "[\"Senate Judiciary Committee Chairman\", \"Joseph R. Biden\", \"D-Del.\"]\n",
      "[\"said\", \"Joseph R. Biden\", \"plans to hold four hearings\"]\n",
      "[\"on anti-flag burning proposals\", \"Joseph R. Biden\", \"hearings\"]\n",
      "String 11:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Manygate Management said Ogdon died peacefully after going into a coma following his admission to London 's Charing Cross Hospital Monday for bronchopneumonia.} \"\n",
      "\n",
      "Possible extracts: [\"Manygate Management\", \"said\", \"Ogdon died\"], [\"Ogdon\", \"died\", \"peacefully\"], [\"following his admission\", \"to London 's Charing Cross Hospital\"], [\"London 's Charing Cross Hospital\", \"for\", \"bronchopneumonia.\"]\n",
      "String 12:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Michael Harrington's intellectual energy, dynamism, and social commitment enriched an entire generation, \"_ City University Chancellor Joseph S. Murphy said in the statement.\"}\n",
      "String 13:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Wharf Holdings is a major Hong Kong property development group which holds a 28 percent interest\"}\n",
      "\n",
      "Or more specifically, if you want the triplets to be in the format [Subject, Relation, Object]:\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"[Wharf Holdings, HOLDS, 28 percent interest]\", \"[Wharf Holdings, IS, a major Hong Kong property development group]\", \"[28 percent interest, IS, held by], [Wharf Holdings, WITH, 28 percent]\"}\n",
      "String 14:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Lawrence Kane Jr. of Cincinnati, Ohio's special prosecutor in the Home State case, declined comment on any settlement possibilities.\")\n",
      "\n",
      "Triplets extracted from the given text:\n",
      "\n",
      "1. (Lawrence Kane Jr., is, special prosecutor)\n",
      "2. (Lawrence Kane Jr., is from, Cincinnati)\n",
      "3. (Lawrence Kane Jr., is in, Ohio)\n",
      "4. (Lawrence Kane Jr., is involved in, the Home State case)\n",
      "5. (He, is, special prosecutor)\n",
      "6. (He, is from, Cincinnati)\n",
      "7. (He, is in, Ohio)\n",
      "8. (He, is involved in, the Home State case)\n",
      "9. (Special prosecutor, declines, comment)\n",
      "10. (Special prosecutor, on, settlement possibilities)\n",
      "String 15:  {\"action\": \"extract_text_triplets\", \"action_input\": \"This is the great lurking problem in all technology transfers,\" \"says Steven Bryen,\" \"a former Pentagon official responsible for U.S. technology transfers.\"}\n",
      "\n",
      "This input would yield the following triplet: [\"This is the great lurking problem in all technology transfers\", \"says\", \"Steven Bryen\"]\n",
      "[\"This is the great lurking problem in all technology transfers\", \"is said by\", \"Steven Bryen\"]\n",
      "[\"Steven Bryen\", \"said\", \"This is the great lurking problem in all technology transfers\"]\n",
      "[\"Steven Bryen\", \"is\", \"a former Pentagon official\"]\n",
      "[\"a former Pentagon official\", \"is responsible for\", \"US technology transfers\"]\n",
      "String 16:  {\"action\": \"extract_text_triplets\", \"action_input\": \"( Article by S. Gusak, deputy chairman of the Belarusian ________, ________}\"}\n",
      "\n",
      "You will need to fill in the blanks with appropriate entities or entities types based on the context. For example, if the context suggests that the Deputy Chairman is a politician, then the blanks could be filled as follows:\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"( Article by S. Gusak, deputy chairman of the Belarusian Government, Belarus)\"}\n",
      "String 17:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Rio de Janeiro is the location of O GLOBO\"}\n",
      "\n",
      "or if you mean the relationship between the two: {\"action\": \"extract_text_triplets\", \"action_input\": \"O GLOBO is based in Rio de Janeiro\"}\n",
      "String 18:  {\"action\": \"extract_text_triplets\", \"action_input\": \"\"This agreement is extremely important,\" said Ortega, after signing the agreement with Rene Steichen, European agricultural official, at the EU headquarters in Brussels.\"\"}\n",
      "\n",
      "This will return triplets related to the given text, such as: (Ortega said, This agreement is extremely important, after signing), (Ortega, said), (This agreement is extremely important, after signing), (signed the agreement, Ortega with Rene Steichen), (Ortega, signed the agreement), (at the EU headquarters in Brussels, the agreement was signed), (in Brussels, the agreement was signed), etc.\n",
      "String 19:  {\"action\": \"extract_text_triplets\", \"action\\_input\": \"The LAPD investigation into the Robert F. Kennedy assassination by Sirhan Sirhan was exhaustive, yet public concern about its conclusions remains 20 years later, ' Eu said in a written statement released by her office.\"}\n",
      "String 20:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Twenty-five years after the assassination of President John F. Kennedy, Lee Harvey Oswald's widow says she now believes Oswald did not act alone in the killing.\")\n",
      "String 21:  {\"action\": \"extract_text_tripleps\", \"action_input\": \"Sen. Robert F. Kennedy's assassin, Sirhan B. Sirhan, has an unpredictable capacity for violence and remains a threat to society, a state prison board ruled in denying him parole for the 10th time.\"}\n",
      "String 22:  {\"action\": \"extract_text_triplets\", \"action_input\": \"James Earl Ray, serving a 99-year prison sentence for killing civil rights leader Martin Luther King Jr., has filed for divorce from his wife of 12 years.}\")\n",
      "\n",
      "Triplets: [[\"James Earl Ray\", \"serving\", \"prison sentence\"], [\"James Earl Ray\", \"for\", \"killing\"], [\"James Earl Ray\", \"has filed\", \"divorce\"], [\"he\", \"is\", \"serving\"], [\"he\", \"for\", \"killing\"], [\"he\", \"has filed\", \"divorce\"], [\"James Earl Ray\", \"from\", \"his wife\"], [\"he\", \"of\", \"12 years\"], [\"his wife\", \"of\", \"12 years\"]]\n",
      "String 23:  {\"action\": \"extract_text_triplets\", \"action_input\": \"\" Sirhan had killed Kennedy for his warm feelings toward Israel , and I had come from Israel , \" Markman said .\"}\n",
      "\n",
      "This text seems to imply the following triplets:\n",
      "\n",
      "1. Sirhan killed Kennedy because of his warm feelings toward Israel.\n",
      "2. I had come from Israel.\n",
      "String 24:  {\"action\": \"extract_text_triplets\", \"action\\_input\": \"Ruby shot Oswald to death with the .38-caliber Colt Cobra revolver in the basement of Dallas City Jail on Nov. 24, 1963, two days after President Kennedy was assassinated .\"}\n",
      "String 25:  {\"action\": \"extract_text_triplets\", \"action_input\": \"and former CBS News commentator Eric Sevareid, who was born in Velva, several miles southeast of Minot.} \"\n",
      "\n",
      "Triplets extracted: [\"Eric Sevareid\", \"was born\", \"Velva, several miles southeast of Minot.\"] [\"and former CBS News commentator\", \"who\", \"Eric Sevareid\"] [\"who\", \"was born\", \"Velva, several miles southeast of Minot.\"] [\"Eric Sevareid\", \"is\", \"former CBS News commentator.\"] [\"Velva, several miles southeast of Minot\", \"is the place of birth for\", \"Eric Sevareid.\"] [\"who\", \"is\", \"and former CBS News commentator\"] [\"Eric Sevareid\", \"was\", \"a commentator for CBS News.\"] [\"several miles southeast of Minot\", \"is the location of\", \"Velva.\"] [\"Velva\", \"is the place of birth for\", \"who\"] [\"and former CBS News commentator\", \"was born in\", \"Velva, several miles southeast of Minot.\"] [\"is the place of birth for\", \"Eric Sevareid\", \"Velva, several miles southeast of Minot.\"] [\"Velva, several miles southeast of Minot\", \"is located\", \"several miles southeast of Minot.\"] [\"several miles southeast of Minot\", \"is the location of\", \"Velva.\"] [\"Velva\", \"is the birthplace of\", \"who\"] [\"who\", \"is\", \"and former CBS News commentator Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\", \"Velva\"] [\"Velva\", \"is the birthplace of\", \"and former CBS News commentator\"] [\"is the birthplace of\", \"Eric Sevareid\", \"Velva\"] [\"Velva\", \"is the location of\", \"several miles southeast of Minot.\"] [\"several miles southeast of Minot\", \"is the location of\", \"Velva, several miles southeast of Minot.\"] [\"Velva\", \"is the place of birth for\", \"Eric Sevareid, several miles southeast of Minot.\"] [\"Eric Sevareid\", \"is the person who\", \"was born in Velva, several miles southeast of Minot.\"] [\"was born in\", \"Velva, several miles southeast of Minot\", \"Eric Sevareid\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"and former CBS News commentator\"] [\"and former CBS News commentator\", \"was born in\", \"Velva, several miles southeast of Minot\"] [\"Eric Sevareid\", \"was born in Velva\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"is the birthplace of\", \"Eric Sevareid\", \"Velva, several miles southeast of Minot.\"] [\"Velva\", \"is the location of\", \"several miles southeast of Minot, where Eric Sevareid was born.\"] [\"several miles southeast of Minot, where Eric Sevareid was born\", \"is the location of\", \"Velva\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid, a former CBS News commentator.\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot, and was a former CBS News commentator.\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid, who was a former CBS News commentator.\"] [\"Eric Sevareid\", \"was born in Velva\", \"and was a former CBS News commentator.\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid, who was born there.\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\", \"Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"and Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\", \"Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\n",
      "String 26:  {\"action\": \"extract_text_triplets\", \"action_input\": \"They plan to resubmit their proposal, and for the moment have pledged that the St. Louis Street side be dedicated to native New Orleans musicians such as Fats Domino,\" \"said spokeswoman Barbara Hutson in a news release Friday.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the pred.json file\n",
    "with open('pred_conll04.json', 'r') as file:\n",
    "    pred_data = json.load(file)\n",
    "\n",
    "# Initialize a counter for entries with \"triples\" as a string\n",
    "string_triples_count = 0\n",
    "string_triples = []\n",
    "\n",
    "# Iterate over the entries and check the type of \"triples\"\n",
    "for entry in pred_data:\n",
    "    if 'triples' in entry and isinstance(entry['triples'], str):\n",
    "        string_triples_count += 1\n",
    "        string_triples.append(entry['triples'])\n",
    "\n",
    "# Print the count of such entries\n",
    "\n",
    "print(f\"Number of entries with 'triples' as a string: {string_triples_count}\")\n",
    "\n",
    "for i, string in enumerate(string_triples, start=1):\n",
    "    print(f\"String {i}: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hallucination = 26/288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09027777777777778"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open('test_triples_conll04.json', 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open('pred_conll04.json', 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Find the IDs of entries with \"triples\" as a string in file1\n",
    "ids_to_remove = [entry['id'] for entry in data2 if 'triples' in entry and isinstance(entry['triples'], str)]\n",
    "\n",
    "# Remove the entries from both files\n",
    "filtered_data1 = [entry for entry in data1 if entry['id'] not in ids_to_remove]\n",
    "filtered_data2 = [entry for entry in data2 if entry['id'] not in ids_to_remove]\n",
    "\n",
    "# Save the filtered data back to new JSON files\n",
    "with open('Finaltest.json', 'w') as file:\n",
    "    json.dump(filtered_data1 , file, indent=4)\n",
    "\n",
    "with open('Finalpred.json', 'w') as file:\n",
    "    json.dump(filtered_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the entries have been removed\n",
    "print(f\"Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_scores(tp, total_golden, total_prediction):\n",
    "    precision = tp / total_prediction if total_prediction > 0 else 0\n",
    "    recall = tp / total_golden if total_golden > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Function to process the files and calculate the scores, considering extras\n",
    "def evaluate_predictions_corrected(golden_file, prediction_file):\n",
    "    # Load the golden truths and predictions\n",
    "    with open(golden_file, 'r') as f:\n",
    "        golden_data = json.load(f)\n",
    "    with open(prediction_file, 'r') as f:\n",
    "        prediction_data = json.load(f)\n",
    "\n",
    "    tp = 0\n",
    "    extras = 0\n",
    "\n",
    "    # Convert golden data and prediction data into dictionaries for easier access\n",
    "    golden_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in golden_data}\n",
    "    prediction_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in prediction_data}\n",
    "\n",
    "    # Iterate over each instance in the golden data to calculate true positives\n",
    "    for id, golden_triples in golden_dict.items():\n",
    "        prediction_triples = prediction_dict.get(id, set())\n",
    "        tp += len(golden_triples & prediction_triples)\n",
    "\n",
    "    # Calculate extras in prediction\n",
    "    for id, prediction_triples in prediction_dict.items():\n",
    "        if id not in golden_dict:\n",
    "            extras += len(prediction_triples)\n",
    "        else:\n",
    "            unmatched_triples = prediction_triples - golden_dict[id]\n",
    "            extras += len(unmatched_triples)\n",
    "\n",
    "    # Calculate micro scores\n",
    "    total_golden = sum(len(triples) for triples in golden_dict.values())\n",
    "    total_prediction = sum(len(triples) for triples in prediction_dict.values())\n",
    "    precision_micro, recall_micro, f1_micro = calculate_scores(tp, total_golden, total_prediction)\n",
    "\n",
    "    # Calculate macro scores\n",
    "    total_items = len(golden_dict)\n",
    "    precision_macro, recall_macro, f1_macro = 0, 0, 0\n",
    "    for id, golden_triples in golden_dict.items():\n",
    "        prediction_triples = prediction_dict.get(id, set())\n",
    "        tp = len(golden_triples & prediction_triples)\n",
    "        precision, recall, _ = calculate_scores(tp, len(golden_triples), len(prediction_triples))\n",
    "        precision_macro += precision\n",
    "        recall_macro += recall\n",
    "    precision_macro /= total_items\n",
    "    recall_macro /= total_items\n",
    "    f1_macro = 2 * (precision_macro * recall_macro) / (precision_macro + recall_macro) if (precision_macro + recall_macro) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'micro': {\n",
    "            'precision': precision_micro,\n",
    "            'recall': recall_micro,\n",
    "            'f1': f1_micro\n",
    "        },\n",
    "        'macro': {\n",
    "            'precision': precision_macro,\n",
    "            'recall': recall_macro,\n",
    "            'f1': f1_macro\n",
    "        },\n",
    "        'true_positives': tp,\n",
    "        'extras': extras\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Scores: {'precision': 0.042328042328042326, 'recall': 0.042440318302387266, 'f1': 0.0423841059602649}\n",
      "Macro Scores: {'precision': 0.05216284987277353, 'recall': 0.04961832061068702, 'f1': 0.0508587786259542}\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_predictions_corrected('Finaltest.json', 'Finalpred.json')\n",
    "print(\"Micro Scores:\", scores['micro'])\n",
    "print(\"Macro Scores:\", scores['macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_scores(tp, total_golden, total_prediction):\n",
    "    precision = tp / total_prediction if total_prediction > 0 else 0\n",
    "    recall = tp / total_golden if total_golden > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Function to process the files and calculate the scores, considering extras\n",
    "def evaluate_predictions_corrected(golden_file, prediction_file):\n",
    "    # Load the golden truths and predictions\n",
    "    with open(golden_file, 'r') as f:\n",
    "        golden_data = json.load(f)\n",
    "    with open(prediction_file, 'r') as f:\n",
    "        prediction_data = json.load(f)\n",
    "\n",
    "    tp = 0\n",
    "    extras = 0\n",
    "\n",
    "    # Convert golden data and prediction data into dictionaries for easier access\n",
    "    golden_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in golden_data}\n",
    "    prediction_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in prediction_data}\n",
    "\n",
    "    # Iterate over each instance in the golden data to calculate true positives\n",
    "    for id, golden_triples in golden_dict.items():\n",
    "        prediction_triples = prediction_dict.get(id, set())\n",
    "        tp += len(golden_triples & prediction_triples)\n",
    "\n",
    "    # Calculate extras in prediction\n",
    "    for id, prediction_triples in prediction_dict.items():\n",
    "        if id not in golden_dict:\n",
    "            extras += len(prediction_triples)\n",
    "        else:\n",
    "            unmatched_triples = prediction_triples - golden_dict[id]\n",
    "            extras += len(unmatched_triples)\n",
    "\n",
    "    # Calculate scores\n",
    "    precision, recall, f1 = calculate_scores(tp, sum(len(triples) for triples in golden_dict.values()), sum(len(triples) for triples in prediction_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'true_positives': tp,\n",
    "        'extras': extras\n",
    "    }\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_predictions_corrected('Finaltest.json', 'Finalpred.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.042328042328042326,\n",
       " 'recall': 0.042440318302387266,\n",
       " 'f1': 0.0423841059602649,\n",
       " 'true_positives': 16,\n",
       " 'extras': 362}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
