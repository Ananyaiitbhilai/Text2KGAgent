{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/ccc66ybs0m59z0yyhly4_jp80000gn/T/ipykernel_22455/3471624335.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n",
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLM locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3877.58 MiB, ( 8565.05 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3877.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 8825.05 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, ( 8989.06 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf\",  \n",
    "n_ctx=2048,\n",
    "n_gpu_layers=-1,\n",
    "n_batch=512,\n",
    "callback_manager=callback_manager,\n",
    "verbose=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Triple extraction tool (based on REBEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline and tokenizer once\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "def extract_text_triplets(input_text):\n",
    "    \"\"\"\n",
    "    Extracts triplets from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The text from which to extract triplets.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each representing a triplet with 'head', 'type', and 'tail'.\n",
    "    \"\"\"\n",
    "    # Use the tokenizer manually since we need special tokens\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode([\n",
    "        triplet_extractor(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]\n",
    "    ])\n",
    "\n",
    "    # Function to parse the generated text and extract the triplets\n",
    "    def extract_triplets(text):\n",
    "        triplets = []\n",
    "        relation, subject, object_ = '', '', ''\n",
    "        text = text.strip()\n",
    "        current = 'x'\n",
    "        for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "            if token == \"<triplet>\":\n",
    "                current = 't'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                    relation = ''\n",
    "                subject = ''\n",
    "            elif token == \"<subj>\":\n",
    "                current = 's'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                object_ = ''\n",
    "            elif token == \"<obj>\":\n",
    "                current = 'o'\n",
    "                relation = ''\n",
    "            else:\n",
    "                if current == 't':\n",
    "                    subject += ' ' + token\n",
    "                elif current == 's':\n",
    "                    object_ += ' ' + token\n",
    "                elif current == 'o':\n",
    "                    relation += ' ' + token\n",
    "        if subject != '' and relation != '' and object_ != '':\n",
    "            triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "        return triplets\n",
    "\n",
    "    extracted_triplets = extract_triplets(extracted_text[0])\n",
    "    return extracted_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract triples for each context in eval.json and create pred.json\n",
    "def generate_pred_json(eval_file_path, pred_file_path):\n",
    "    # Load the evaluation data from eval.json\n",
    "    with open(eval_file_path, 'r') as file:\n",
    "        eval_data = json.load(file)\n",
    "    \n",
    "    # Initialize a list to hold the modified data with extracted triples\n",
    "    modified_data = []\n",
    "    \n",
    "    # Iterate over each item in the evaluation data\n",
    "    for item in eval_data:\n",
    "        context = item['context']\n",
    "        # Prepare the command with the contex\n",
    "        # Use the process_command function to predict the extracted triples\n",
    "        extracted_t = extract_text_triplets(context)\n",
    "        # Append the extracted triples to the item under the 'triples' key\n",
    "        item['triples'] =  extracted_t \n",
    "        # Append the modified item to the modified_data list\n",
    "        modified_data.append(item)\n",
    "    \n",
    "    # Write the modified data with extracted triples to pred.json\n",
    "    with open(pred_file_path, 'w') as file:\n",
    "        json.dump(modified_data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "eval_file_path = '/Users/ananyahooda/Desktop/final/data/evaluation_data/conll04_eval.json' # Replace with the actual path to your eval.json file\n",
    "pred_file_path = '/Users/ananyahooda/Desktop/final/pred_conll04.json' # The output file path\n",
    "generate_pred_json(eval_file_path, pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': 'Hakawati Theatre',\n",
       "  'type': 'located in the administrative territorial entity',\n",
       "  'tail': 'Jerusalem'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_triplets(\"An art exhibit at the Hakawati Theatre in Arab east Jerusalem was a series of portraits of Palestinians killed in the rebellion .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Below are two examples where context is the text and triples are the corresponding extracted triples:\n",
    "{{\n",
    "        \"context\": \"John Wilkes Booth , who assassinated President Lincoln , was an actor .\n",
    "                \"triples\": [\n",
    "            {{\"head\": \"John Wilkes Booth\",\"type\": \"killed by\",\"tail\": \"President Lincoln\"}}\n",
    "        ]\n",
    "    }},\n",
    "{{\n",
    "        \"context\": \"Marie Magdefrau Ferraro , 50 , of Bethany , Conn. , was shot to death Thursday when two bandits armed with assault rifles emerged from nearby bushes and began firing at a van carrying a Connecticut Audubon Society wildlife wild tour group .\",\n",
    "        \"triples\": [\n",
    "            {{\"head\": \"Marie Magdefrau Ferraro\", \"type\": \"residence\",\"tail\": \"Bethany\"}},\n",
    "            {{\"head\": \"Marie Magdefrau Ferraro\", \"type\": \"residence\", \"tail\": \"Conn.\"}},\n",
    "            {{\"head\": \"Bethany\",\"type\": \"location\", \"tail\": \"Conn.\"}}\n",
    "        ]\n",
    "}}\n",
    "\n",
    "Here are the types/relations which are allowed: {{\"killed by\", \"residence\", \"location\", \"headquarters\", \"location\", \"employer\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''<s>[INST] <<SYS>>\n",
    "Assistant is an expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to trigger actions for User by responding with JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "The available action to Assistant is:\n",
    "- \"extract_text_triplets\": Useful for when Assistant is asked to extract triplets from a given text.\n",
    "  - To use the extract_triplets tool, Assistant should respond like so:\n",
    "    {{\"action\": \"extract_text_triplets\", \"action_input\": \"Your text here\"}}\n",
    "\n",
    "Assistant will only output the following relations: \"killed by\", \"residence\", \"location\", \"headquarters location\", \"employer\".\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: I'm good thanks, how are you?\n",
    "User: Can you extract all the triplets from this text: \"Gràcia is a district of the city of Barcelona, Spain.\"\n",
    "Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"Gràcia is a district of the city of Barcelona, Spain.\"}}\n",
    "User: Also give triples for \"obama was US president\"\n",
    "Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"obama was US president\"}}\n",
    "\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{0}[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating single-tool with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_command(command):\n",
    "    # Put user command into prompt\n",
    "    prompt = prompt_template.format(\"User: \" + command)\n",
    "    # Send command to the model\n",
    "    output = llm(prompt, max_tokens=2000, stop=[\"User:\"])\n",
    "    response = output['choices'][0]['text']\n",
    "\n",
    "    # try to find json in the response\n",
    "    try:\n",
    "        # Extract json from model response by finding first and last brackets {}\n",
    "        firstBracketIndex = response.index(\"{\")\n",
    "        lastBracketIndex = len(response) - response[::-1].index(\"}\")\n",
    "        jsonString = response[firstBracketIndex:lastBracketIndex]\n",
    "        responseJson = json.loads(jsonString)\n",
    "        if responseJson['action'] == 'extract_text_triplets':\n",
    "            extracted_triplets = extract_text_triplets(responseJson['action_input'])\n",
    "            return extracted_triplets   \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # No json match, just return response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4964.28 ms\n",
      "llama_print_timings:      sample time =       7.61 ms /    30 runs   (    0.25 ms per token,  3942.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2304.51 ms /   205 tokens (   11.24 ms per token,    88.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2463.19 ms /    29 runs   (   84.94 ms per token,    11.77 tokens per second)\n",
      "llama_print_timings:       total time =    4895.35 ms /   234 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'Ananya', 'type': 'educated at', 'tail': 'IIT Bhilai'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Extract triples for:\\\"Ananya is working at IIT Bhilai\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Prediction files for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4964.28 ms\n",
      "llama_print_timings:      sample time =      17.49 ms /    62 runs   (    0.28 ms per token,  3544.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     428.34 ms /    21 tokens (   20.40 ms per token,    49.03 tokens per second)\n",
      "llama_print_timings:        eval time =    5295.25 ms /    61 runs   (   86.81 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =    6008.08 ms /    82 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 2 column 1 (char 84)\n",
      "Extracted triples have been saved to out.json\n"
     ]
    }
   ],
   "source": [
    "extracted_triplets = process_command(\"Can you please give triple for \\\"Ananya works for IIT Bhilai.\\\"\")\n",
    "\n",
    "# Save the extracted triples to a JSON file\n",
    "output_file_path = 'out.json'  # Define the output file path\n",
    "\n",
    "# Write the extracted triples to the output file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    json.dump(extracted_triplets, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the file has been saved\n",
    "print(f\"Extracted triples have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\"Loc\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m eval_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/ananyahooda/Desktop/final/data/evaluation_data/conll04_eval.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Replace with the actual path to your eval.json file\u001b[39;00m\n\u001b[1;32m     30\u001b[0m pred_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/ananyahooda/Desktop/final/pred_conll04.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# The output file path\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mgenerate_pred_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mgenerate_pred_json\u001b[0;34m(eval_file_path, pred_file_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you please give triples for \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Use the process_command function to predict the extracted triples\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m extracted_triplets \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Append the extracted triples to the item under the 'triples' key\u001b[39;00m\n\u001b[1;32m     20\u001b[0m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriples\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m extracted_triplets\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mprocess_command\u001b[0;34m(command)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_command\u001b[39m(command):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Put user command into prompt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Send command to the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m llm(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser:\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: '\"Loc\"'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract triples for each context in eval.json and create pred.json\n",
    "def generate_pred_json(eval_file_path, pred_file_path):\n",
    "    # Load the evaluation data from eval.json\n",
    "    with open(eval_file_path, 'r') as file:\n",
    "        eval_data = json.load(file)\n",
    "    \n",
    "    # Initialize a list to hold the modified data with extracted triples\n",
    "    modified_data = []\n",
    "    \n",
    "    # Iterate over each item in the evaluation data\n",
    "    for item in eval_data:\n",
    "        context = item['context']\n",
    "        # Prepare the command with the context\n",
    "        command = f\"Can you please give triples for {context}\"\n",
    "        # Use the process_command function to predict the extracted triples\n",
    "        extracted_triplets = process_command(command)\n",
    "        # Append the extracted triples to the item under the 'triples' key\n",
    "        item['triples'] = extracted_triplets\n",
    "        # Append the modified item to the modified_data list\n",
    "        modified_data.append(item)\n",
    "    \n",
    "    # Write the modified data with extracted triples to pred.json\n",
    "    with open(pred_file_path, 'w') as file:\n",
    "        json.dump(modified_data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "eval_file_path = '/Users/ananyahooda/Desktop/final/data/evaluation_data/conll04_eval.json' # Replace with the actual path to your eval.json file\n",
    "pred_file_path = '/Users/ananyahooda/Desktop/final/pred_conll04.json' # The output file path\n",
    "generate_pred_json(eval_file_path, pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the golden truth JSON file: 288\n",
      "Length of the prediction JSON file: 288\n",
      "Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open('pred_conll04_Mistral.json', 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open('pred_conll04.json', 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Determine the length of both JSON files\n",
    "length_data1 = len(data1)\n",
    "length_data2 = len(data2)\n",
    "\n",
    "# Print the lengths\n",
    "print(f\"Length of the golden truth JSON file: {length_data1}\")\n",
    "print(f\"Length of the prediction JSON file: {length_data2}\")\n",
    "\n",
    "# Convert the lists to dictionaries indexed by the 'id' attribute\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "data2_dict = {item['id']: item for item in data2}\n",
    "\n",
    "# Find the common IDs\n",
    "common_ids = set(data1_dict.keys()) & set(data2_dict.keys())\n",
    "\n",
    "# Extract the common data points\n",
    "common_data1 = [data1_dict[id] for id in common_ids]\n",
    "common_data2 = [data2_dict[id] for id in common_ids]\n",
    "\n",
    "# Save the common data points to new JSON files\n",
    "with open('pred_conll04_Mistral.json', 'w') as file:\n",
    "    json.dump(common_data1, file, indent=4)\n",
    "\n",
    "with open('pred_conll04.json', 'w') as file: \n",
    "    json.dump(common_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the new files have been created\n",
    "print(f\"Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with 'triples' as a string: 26\n",
      "String 1:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Of 40 million party line calls logged by New England Telephone over a 2-year period, at least 10 percent were made by those who dial services like XBT Telecom's 'Talkabout' teen line, a group conversation designed for people under age 16, said John Johnson, a spokesman for the telephone company.}\n",
      "\n",
      "Possible triplets from the text:\n",
      "\n",
      "1. [\"New England Telephone\", \"logged\", \"40 million party line calls\"]\n",
      "2. [\"Over a 2-year period\", \"at least\", \"10 percent\"]\n",
      "3. [\"People who dial services like XBT Telecom's 'Talkabout'\", \"made\", \"party line calls\"]\n",
      "4. [\"XBT Telecom's 'Talkabout'\", \"is\", \"a group conversation\"]\n",
      "5. [\"People under age 16\", \"designed for\", \"XBT Telephone's 'Talkabout'\"]\n",
      "6. [\"John Johnson\", \"said\", \"spokesman for the telephone company\"]\n",
      "String 2:  {\"action\": \"extract_text_triplets\", \"action_input\": \"They plan to resubmit their proposal, and for the moment have pledged that the St. Louis Street side be dedicated to native New Orleans musicians such as Fats Domino,\" \"said spokeswoman Barbara Hutson in a news release Friday.\"}\n",
      "String 3:  {\"action\": \"extract_text_triplets\", \"action_input\": \"\" Sirhan had killed Kennedy for his warm feelings toward Israel , and I had come from Israel , \" Markman said .\"}\n",
      "\n",
      "This text seems to imply the following triplets:\n",
      "\n",
      "1. Sirhan killed Kennedy because of his warm feelings toward Israel.\n",
      "2. I had come from Israel.\n",
      "String 4:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Rio de Janeiro is the location of O GLOBO\"}\n",
      "\n",
      "or if you mean the relationship between the two: {\"action\": \"extract_text_triplets\", \"action_input\": \"O GLOBO is based in Rio de Janeiro\"}\n",
      "String 5:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Twenty-five years after the assassination of President John F. Kennedy, Lee Harvey Oswald's widow says she now believes Oswald did not act alone in the killing.\")\n",
      "String 6:  {\"action\": \"extract_text_triplets\", \"action\\_input\": \"The LAPD investigation into the Robert F. Kennedy assassination by Sirhan Sirhan was exhaustive, yet public concern about its conclusions remains 20 years later, ' Eu said in a written statement released by her office.\"}\n",
      "String 7:  {\"action\": \"extract_text_triplets\", \"action_input\": \"and former CBS News commentator Eric Sevareid, who was born in Velva, several miles southeast of Minot.} \"\n",
      "\n",
      "Triplets extracted: [\"Eric Sevareid\", \"was born\", \"Velva, several miles southeast of Minot.\"] [\"and former CBS News commentator\", \"who\", \"Eric Sevareid\"] [\"who\", \"was born\", \"Velva, several miles southeast of Minot.\"] [\"Eric Sevareid\", \"is\", \"former CBS News commentator.\"] [\"Velva, several miles southeast of Minot\", \"is the place of birth for\", \"Eric Sevareid.\"] [\"who\", \"is\", \"and former CBS News commentator\"] [\"Eric Sevareid\", \"was\", \"a commentator for CBS News.\"] [\"several miles southeast of Minot\", \"is the location of\", \"Velva.\"] [\"Velva\", \"is the place of birth for\", \"who\"] [\"and former CBS News commentator\", \"was born in\", \"Velva, several miles southeast of Minot.\"] [\"is the place of birth for\", \"Eric Sevareid\", \"Velva, several miles southeast of Minot.\"] [\"Velva, several miles southeast of Minot\", \"is located\", \"several miles southeast of Minot.\"] [\"several miles southeast of Minot\", \"is the location of\", \"Velva.\"] [\"Velva\", \"is the birthplace of\", \"who\"] [\"who\", \"is\", \"and former CBS News commentator Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\", \"Velva\"] [\"Velva\", \"is the birthplace of\", \"and former CBS News commentator\"] [\"is the birthplace of\", \"Eric Sevareid\", \"Velva\"] [\"Velva\", \"is the location of\", \"several miles southeast of Minot.\"] [\"several miles southeast of Minot\", \"is the location of\", \"Velva, several miles southeast of Minot.\"] [\"Velva\", \"is the place of birth for\", \"Eric Sevareid, several miles southeast of Minot.\"] [\"Eric Sevareid\", \"is the person who\", \"was born in Velva, several miles southeast of Minot.\"] [\"was born in\", \"Velva, several miles southeast of Minot\", \"Eric Sevareid\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"and former CBS News commentator\"] [\"and former CBS News commentator\", \"was born in\", \"Velva, several miles southeast of Minot\"] [\"Eric Sevareid\", \"was born in Velva\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"is the birthplace of\", \"Eric Sevareid\", \"Velva, several miles southeast of Minot.\"] [\"Velva\", \"is the location of\", \"several miles southeast of Minot, where Eric Sevareid was born.\"] [\"several miles southeast of Minot, where Eric Sevareid was born\", \"is the location of\", \"Velva\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid, a former CBS News commentator.\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot, and was a former CBS News commentator.\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid, who was a former CBS News commentator.\"] [\"Eric Sevareid\", \"was born in Velva\", \"and was a former CBS News commentator.\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid, who was born there.\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\", \"Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"and Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\", \"Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva, several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in Velva\", \"Velva is several miles southeast of Minot\"] [\"Velva, several miles southeast of Minot\", \"is the birthplace of\", \"Eric Sevareid\"] [\"Eric Sevareid\", \"was born in\n",
      "String 8:  {\"action\": \"extract_text_triplets\", \"action\\_input\": \"The old bugaboo is where this baby is going to hit,\" \" said John Jamison, a National Weather Service meteorologist in Galveston.\"}\n",
      "\n",
      "This will extract subject-verb-object triplets from the given text, such as [\"John Jamison\", \"said\", \"was going to hit\"] or [\"the old bugaboo\", \"is\", \"where this baby is going to hit\"].\n",
      "String 9:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Bush, who acknowledged that Sununu was not quiet and retiring also had words for Virginia's Baliles, who was stepping down after a year as chairman of the association.\")\n",
      "}\n",
      "\n",
      "Triplets: [[\"Bush\", \"had words for\", \"Baliles\"], [\"Baliles\", \"was stepping down\", \"after a year\"], [\"Sununu\", \"was not quiet and retiring\"]]\n",
      "String 10:  {\"action\": \"extract_text_triplets\", \"action_input\": \"But Jack Frazier, Rotary Club president, said volunteers picked up the ducks and all but four or five were accounted for.}\n",
      "\n",
      "Possible extracts: [\n",
      "{\"subject\": \"But\", \"predicate\": \"said\", \"object\": \"Jack Frazier\"},\n",
      "{\"subject\": \"Jack Frazier\", \"predicate\": \"was\", \"object\": \"Rotary Club president\"},\n",
      "{\"subject\": \"volunteers\", \"predicate\": \"picked\", \"object\": \"up the ducks\"},\n",
      "{\"subject\": \"all but four or five\", \"predicate\": \"were\", \"object\": \"accounted for.\"}]\n",
      "String 11:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Senate Judiciary Committee Chairman Joseph R. Biden, D-Del., said he plans to hold four hearings on anti-flag burning proposals.}\n",
      "\n",
      "Triplets:\n",
      "[\"Senate Judiciary Committee Chairman\", \"Joseph R. Biden\", \"D-Del.\"]\n",
      "[\"said\", \"Joseph R. Biden\", \"plans to hold four hearings\"]\n",
      "[\"on anti-flag burning proposals\", \"Joseph R. Biden\", \"hearings\"]\n",
      "String 12:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Lawrence Kane Jr. of Cincinnati, Ohio's special prosecutor in the Home State case, declined comment on any settlement possibilities.\")\n",
      "\n",
      "Triplets extracted from the given text:\n",
      "\n",
      "1. (Lawrence Kane Jr., is, special prosecutor)\n",
      "2. (Lawrence Kane Jr., is from, Cincinnati)\n",
      "3. (Lawrence Kane Jr., is in, Ohio)\n",
      "4. (Lawrence Kane Jr., is involved in, the Home State case)\n",
      "5. (He, is, special prosecutor)\n",
      "6. (He, is from, Cincinnati)\n",
      "7. (He, is in, Ohio)\n",
      "8. (He, is involved in, the Home State case)\n",
      "9. (Special prosecutor, declines, comment)\n",
      "10. (Special prosecutor, on, settlement possibilities)\n",
      "String 13:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"That original one was knocked down for a reason by the tanks , and I presume this one was knocked down for the same reason , ' Santa Monica artist Tom Van Sant said Monday after the 23-foot-tall statue was found crushed and broken in pieces.\"}\n",
      "\n",
      "Here are the potential triplets from the given text:\n",
      "\n",
      "1. [Santa Monica artist Tom Van Sant, said, Monday]\n",
      "2. [Santa Monica artist Tom Van Sant, presumed, same reason]\n",
      "3. [Original statue, was knocked down, reason]\n",
      "4. [New statue, was found, crushed and broken in pieces]\n",
      "5. [Tom Van Sant, spoke, Monday]\n",
      "String 14:  {\"action\": \"extract_text_triplets\", \"action_input\": \"James Earl Ray, serving a 99-year prison sentence for killing civil rights leader Martin Luther King Jr., has filed for divorce from his wife of 12 years.}\")\n",
      "\n",
      "Triplets: [[\"James Earl Ray\", \"serving\", \"prison sentence\"], [\"James Earl Ray\", \"for\", \"killing\"], [\"James Earl Ray\", \"has filed\", \"divorce\"], [\"he\", \"is\", \"serving\"], [\"he\", \"for\", \"killing\"], [\"he\", \"has filed\", \"divorce\"], [\"James Earl Ray\", \"from\", \"his wife\"], [\"he\", \"of\", \"12 years\"], [\"his wife\", \"of\", \"12 years\"]]\n",
      "String 15:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"In northeastern Oregon, at least five major fires and several smaller ones burned across 60, 000 acres, forcing the evacuation of some rural homes and threatening the watershed for the city of La Grande, authorities said.\"}\n",
      "String 16:  {\"action\": \"extract_text_triplets\", \"action_input\": \"This is the great lurking problem in all technology transfers,\" \"says Steven Bryen,\" \"a former Pentagon official responsible for U.S. technology transfers.\"}\n",
      "\n",
      "This input would yield the following triplet: [\"This is the great lurking problem in all technology transfers\", \"says\", \"Steven Bryen\"]\n",
      "[\"This is the great lurking problem in all technology transfers\", \"is said by\", \"Steven Bryen\"]\n",
      "[\"Steven Bryen\", \"said\", \"This is the great lurking problem in all technology transfers\"]\n",
      "[\"Steven Bryen\", \"is\", \"a former Pentagon official\"]\n",
      "[\"a former Pentagon official\", \"is responsible for\", \"US technology transfers\"]\n",
      "String 17:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"There was no mention of the ` iron triangle ` of members of Congress, the news media and special interest groups who, in a speech to political appointees in Washington on Dec. 13, Reagan claimed had prevented his administration from balancing the federal budget.\"}\n",
      "String 18:  {\"action\": \"extract_text_triplets\", \"action\\_input\": \"Ruby shot Oswald to death with the .38-caliber Colt Cobra revolver in the basement of Dallas City Jail on Nov. 24, 1963, two days after President Kennedy was assassinated .\"}\n",
      "String 19:  {\"action\": \"extract_text_tripleets\", \"action_input\": \"By year 's end , 6 million acres had burned in the West and Alaska , making 1988 the worst fire season in 30 years , and , in terms of firefighting resources committed , the most expensive in U.S. history , Sacher said.\"}\n",
      "\n",
      "Possible extracted triplets:\n",
      "\n",
      "1. (\"Year's end\", \"event\", \"[6 million acres had burned]\")\n",
      "2. (\"West and Alaska\", \"location\", \"[6 million acres had burned]\")\n",
      "3. (\"1988\", \"year\", [\"worse fire season in 30 years\"])\n",
      "4. (\"30 years\", \"duration\", [\"worse fire season in 30 years\"])\n",
      "5. (\"most expensive\", \"adjective\", [\"in U.S. history\"])\n",
      "6. (\"firefighting resources committed\", \"resources\", [\"most expensive in U.S. history\"])\n",
      "7. (\"Sacher said\", \"speaker\", [\". .\"])\n",
      "String 20:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Michael Harrington's intellectual energy, dynamism, and social commitment enriched an entire generation, \"_ City University Chancellor Joseph S. Murphy said in the statement.\"}\n",
      "String 21:  {\"action\": \"extract_text_triplets\", \"action_input\": \"( Article by S. Gusak, deputy chairman of the Belarusian ________, ________}\"}\n",
      "\n",
      "You will need to fill in the blanks with appropriate entities or entities types based on the context. For example, if the context suggests that the Deputy Chairman is a politician, then the blanks could be filled as follows:\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"( Article by S. Gusak, deputy chairman of the Belarusian Government, Belarus)\"}\n",
      "String 22:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Wharf Holdings is a major Hong Kong property development group which holds a 28 percent interest\"}\n",
      "\n",
      "Or more specifically, if you want the triplets to be in the format [Subject, Relation, Object]:\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"[Wharf Holdings, HOLDS, 28 percent interest]\", \"[Wharf Holdings, IS, a major Hong Kong property development group]\", \"[28 percent interest, IS, held by], [Wharf Holdings, WITH, 28 percent]\"}\n",
      "String 23:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Manygate Management said Ogdon died peacefully after going into a coma following his admission to London 's Charing Cross Hospital Monday for bronchopneumonia.} \"\n",
      "\n",
      "Possible extracts: [\"Manygate Management\", \"said\", \"Ogdon died\"], [\"Ogdon\", \"died\", \"peacefully\"], [\"following his admission\", \"to London 's Charing Cross Hospital\"], [\"London 's Charing Cross Hospital\", \"for\", \"bronchopneumonia.\"]\n",
      "String 24:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Aguadilla is a city in Puerto Rico.\"}\n",
      "\n",
      "Or if you meant to provide the triplet for the statement \"Aguadilla is in Puerto Rico\", then the response would be:\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"Aguadilla is in Puerto Rico.\"}\n",
      "String 25:  {\"action\": \"extract_text_triplets\", \"action_input\": \"\"This agreement is extremely important,\" said Ortega, after signing the agreement with Rene Steichen, European agricultural official, at the EU headquarters in Brussels.\"\"}\n",
      "\n",
      "This will return triplets related to the given text, such as: (Ortega said, This agreement is extremely important, after signing), (Ortega, said), (This agreement is extremely important, after signing), (signed the agreement, Ortega with Rene Steichen), (Ortega, signed the agreement), (at the EU headquarters in Brussels, the agreement was signed), (in Brussels, the agreement was signed), etc.\n",
      "String 26:  {\"action\": \"extract_text_tripleps\", \"action_input\": \"Sen. Robert F. Kennedy's assassin, Sirhan B. Sirhan, has an unpredictable capacity for violence and remains a threat to society, a state prison board ruled in denying him parole for the 10th time.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the pred.json file\n",
    "with open('pred_conll04_Mistral.json', 'r') as file:\n",
    "    pred_data = json.load(file)\n",
    "\n",
    "# Initialize a counter for entries with \"triples\" as a string\n",
    "string_triples_count = 0\n",
    "string_triples = []\n",
    "\n",
    "# Iterate over the entries and check the type of \"triples\"\n",
    "for entry in pred_data:\n",
    "    if 'triples' in entry and isinstance(entry['triples'], str):\n",
    "        string_triples_count += 1\n",
    "        string_triples.append(entry['triples'])\n",
    "\n",
    "# Print the count of such entries\n",
    "\n",
    "print(f\"Number of entries with 'triples' as a string: {string_triples_count}\")\n",
    "\n",
    "for i, string in enumerate(string_triples, start=1):\n",
    "    print(f\"String {i}: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open('pred_conll04.json', 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open('pred_conll04_Mistral.json', 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Find the IDs of entries with \"triples\" as a string in file1\n",
    "ids_to_remove = [entry['id'] for entry in data2 if 'triples' in entry and isinstance(entry['triples'], str)]\n",
    "\n",
    "# Remove the entries from both files\n",
    "filtered_data1 = [entry for entry in data1 if entry['id'] not in ids_to_remove]\n",
    "filtered_data2 = [entry for entry in data2 if entry['id'] not in ids_to_remove]\n",
    "\n",
    "# Save the filtered data back to new JSON files\n",
    "with open('golden_truth.json', 'w') as file:\n",
    "    json.dump(filtered_data1 , file, indent=4)\n",
    "\n",
    "with open('prediction.json', 'w') as file:\n",
    "    json.dump(filtered_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the entries have been removed\n",
    "print(f\"Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for calculating Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_scores(tp, total_golden, total_prediction):\n",
    "    precision = tp / total_prediction if total_prediction > 0 else 0\n",
    "    recall = tp / total_golden if total_golden > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Function to process the files and calculate the scores, considering extras\n",
    "def evaluate_predictions_corrected(golden_file, prediction_file):\n",
    "    # Load the golden truths and predictions\n",
    "    with open(golden_file, 'r') as f:\n",
    "        golden_data = json.load(f)\n",
    "    with open(prediction_file, 'r') as f:\n",
    "        prediction_data = json.load(f)\n",
    "\n",
    "    tp = 0\n",
    "    extras = 0\n",
    "\n",
    "    # Convert golden data and prediction data into dictionaries for easier access\n",
    "    golden_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in golden_data}\n",
    "    prediction_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in prediction_data}\n",
    "\n",
    "    # Iterate over each instance in the golden data to calculate true positives\n",
    "    for id, golden_triples in golden_dict.items():\n",
    "        prediction_triples = prediction_dict.get(id, set())\n",
    "        tp += len(golden_triples & prediction_triples)\n",
    "\n",
    "    # Calculate extras in prediction\n",
    "    for id, prediction_triples in prediction_dict.items():\n",
    "        if id not in golden_dict:\n",
    "            extras += len(prediction_triples)\n",
    "        else:\n",
    "            unmatched_triples = prediction_triples - golden_dict[id]\n",
    "            print(unmatched_triples)\n",
    "            extras += len(unmatched_triples)\n",
    "\n",
    "    # Calculate micro scores\n",
    "    total_golden = sum(len(triples) for triples in golden_dict.values())\n",
    "    total_prediction = sum(len(triples) for triples in prediction_dict.values())\n",
    "    precision_micro, recall_micro, f1_micro = calculate_scores(tp, total_golden, total_prediction)\n",
    "\n",
    "    # Calculate macro scores\n",
    "    total_items = len(golden_dict)\n",
    "    precision_macro, recall_macro, f1_macro = 0, 0, 0\n",
    "    for id, golden_triples in golden_dict.items():\n",
    "        prediction_triples = prediction_dict.get(id, set())\n",
    "        tp = len(golden_triples & prediction_triples)\n",
    "        precision, recall, _ = calculate_scores(tp, len(golden_triples), len(prediction_triples))\n",
    "        precision_macro += precision\n",
    "        recall_macro += recall\n",
    "    precision_macro /= total_items\n",
    "    recall_macro /= total_items\n",
    "    f1_macro = 2 * (precision_macro * recall_macro) / (precision_macro + recall_macro) if (precision_macro + recall_macro) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'micro': {\n",
    "            'precision': precision_micro,\n",
    "            'recall': recall_micro,\n",
    "            'f1': f1_micro\n",
    "        },\n",
    "        'macro': {\n",
    "            'precision': precision_macro,\n",
    "            'recall': recall_macro,\n",
    "            'f1': f1_macro\n",
    "        },\n",
    "        'true_positives': tp,\n",
    "        'extras': extras\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'West Germany'), ('type', 'member of'), ('tail', 'AP'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Bingham County'), ('type', 'office held by head of government'), ('tail', \"Sheriff's\")), (('head', \"Sheriff's\"), ('type', 'applies to jurisdiction'), ('tail', 'Bingham County'))}\n",
      "{(('head', 'Yerevan'), ('type', 'located in the administrative territorial entity'), ('tail', 'Armenian')), (('head', 'Armenian'), ('type', 'capital'), ('tail', 'Yerevan'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', \"Maryland's House of Delegates\"), ('type', 'chairperson'), ('tail', 'Judith C. Toth'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Cape Hatteras'), ('type', 'located in the administrative territorial entity'), ('tail', 'N.C.'))}\n",
      "set()\n",
      "set()\n",
      "{(('head', 'American system'), ('type', 'named after'), ('tail', 'We the People'))}\n",
      "{(('head', 'assassination'), ('type', 'participant'), ('tail', 'Sirhan Sirhan'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Panama City'), ('type', 'member of'), ('tail', 'ACAN'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Shoshone-Bannock reservation'), ('type', 'located in the administrative territorial entity'), ('tail', 'Idaho'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Grand Isle'), ('type', 'located in the administrative territorial entity'), ('tail', 'La'))}\n",
      "{(('head', 'Lee Harvey Oswald'), ('type', 'residence'), ('tail', 'Texas School Book Depository')), (('head', 'Texas School Book Depository'), ('type', 'occupant'), ('tail', 'Lee Harvey Oswald'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Bratislava'), ('type', 'located in or next to body of water'), ('tail', 'Gabcikovo')), (('head', 'Gabcikovo'), ('type', 'mouth of the watercourse'), ('tail', 'Danube'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Paris'), ('type', 'country'), ('tail', 'French')), (('head', 'French'), ('type', 'capital'), ('tail', 'Paris'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'CNM'), ('type', 'founded by'), ('tail', 'Jordi Aguil'))}\n",
      "{(('head', 'Okhotsk Sea'), ('type', 'part of'), ('tail', 'Bering Sea')), (('head', 'Bering Sea'), ('type', 'has part'), ('tail', 'Okhotsk Sea'))}\n",
      "set()\n",
      "{(('head', 'London'), ('type', 'twinned administrative body'), ('tail', 'Colorado')), (('head', 'Colorado'), ('type', 'twinned administrative body'), ('tail', 'London'))}\n",
      "{(('head', 'Martin Van Buren'), ('type', 'position held'), ('tail', 'president'))}\n",
      "{(('head', 'Khmer Rouge executions, famine and civil unrest'), ('type', 'country'), ('tail', 'Cambodia'))}\n",
      "{(('head', 'Rutherford B. Hayes'), ('type', 'position held'), ('tail', 'president of the United States'))}\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Hubert H. Humphrey'), ('type', 'place of birth'), ('tail', 'Wallace, S.D.'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Mark David Chapman'), ('type', 'significant event'), ('tail', 'shot and killed him')), (('head', 'shot and killed him'), ('type', 'participant'), ('tail', 'Mark David Chapman'))}\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Liberec'), ('type', 'shares border with'), ('tail', 'Ostved')), (('head', 'Ostved'), ('type', 'shares border with'), ('tail', 'Liberec')), (('head', 'Lesny'), ('type', 'shares border with'), ('tail', 'Ostved')), (('head', 'Ostved'), ('type', 'shares border with'), ('tail', 'Lesny'))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{(('head', \"China's 21st Century Agenda\"), ('type', 'author'), ('tail', 'Deng Nan')), (('head', 'Deng Nan'), ('type', 'notable work'), ('tail', \"China's 21st Century Agenda\"))}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "Micro Scores: {'precision': 0.9074074074074074, 'recall': 0.884020618556701, 'f1': 0.8955613577023498}\n",
      "Macro Scores: {'precision': 0.9226039016115352, 'recall': 0.9206955046649703, 'f1': 0.9216487152415245}\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_predictions_corrected('golden_truth.json', 'prediction.json')\n",
    "print(\"Micro Scores:\", scores['micro'])\n",
    "print(\"Macro Scores:\", scores['macro'])\n",
    "print(scores['extras'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4fa2ca3c54d5fbdec4fbc1ede1009e749a4bbc10aad76919c0852744120220"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
