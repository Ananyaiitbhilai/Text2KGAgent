{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e7374e-1c35-44ae-90b5-f20e21ab62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/ccc66ybs0m59z0yyhly4_jp80000gn/T/ipykernel_18717/3398436372.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "import json\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3be0971-7abb-4c96-872c-e261ce6d5d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3947.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# Create the model using Llama 2 7b chat (runs on CPU)\n",
    "# .bin downloaded from here : https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML\n",
    "# but they can come from here too? : https://ai.meta.com/llama/\n",
    "llm = Llama(model_path=\"/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9319d21e-351a-4118-970e-d4dee5d75a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9591.27 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    16 runs   (    0.14 ms per token,  7366.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9591.16 ms /    11 tokens (  871.92 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:        eval time =   29622.45 ms /    15 runs   ( 1974.83 ms per token,     0.51 tokens per second)\n",
      "llama_print_timings:       total time =   39274.19 ms /    26 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-306248fc-d298-42a2-9c44-683ea1d8c040',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1709435912,\n",
       " 'model': '/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf',\n",
       " 'choices': [{'text': ' Here is a simple way to remember:\\n\\n1. Sunday\\n2.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 11, 'completion_tokens': 16, 'total_tokens': 27}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What are the days of the week in order?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85bfdfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9591.27 ms\n",
      "llama_print_timings:      sample time =       1.95 ms /    16 runs   (    0.12 ms per token,  8226.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5205.79 ms /    12 tokens (  433.82 ms per token,     2.31 tokens per second)\n",
      "llama_print_timings:        eval time =    6264.25 ms /    15 runs   (  417.62 ms per token,     2.39 tokens per second)\n",
      "llama_print_timings:       total time =   11517.27 ms /    27 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-9e88d739-c164-455d-b01f-694b00bb6fd4',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1709436204,\n",
       " 'model': '/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf',\n",
       " 'choices': [{'text': '\\n\\nThe number of days in a month depends on whether it is a common',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 13, 'completion_tokens': 16, 'total_tokens': 29}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"which month has 29 or 28 days?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f72c4723-cd1c-4c62-8c7e-49cafce07777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an prompt template that uses an engineered system_prompt\n",
    "\n",
    "# Based on:\n",
    "# https://www.pinecone.io/learn/llama-2/\n",
    "# and\n",
    "# https://docs.langchain.com/docs/components/agents\n",
    "\n",
    "prompt_template = '''<s>[INST] <<SYS>>\n",
    "Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to trigger actions for User by responding with JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "Actions available to Assistant are:\n",
    "\n",
    "- \"set_room_color\": Useful for when Assistant is asked to set the color of User's lighting.\n",
    "  - To use the set_room_color tool, Assistant should respond like so:\n",
    "    {{\"action\": \"set_room_color\", \"action_input\": \"#FF0000\"}}\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: I'm good thanks, how are you?\n",
    "User: Can you set the color of my room to be red?\n",
    "Assistant: {{\"action\": \"set_room_color\", \"action_input\": \"#FF0000\"}}\n",
    "User: That looks great, but can you set room color to green instead?\n",
    "Assistant: {{\"action\": \"set_room_color\", \"action_input\": \"#00FF00\"}}\n",
    "User: Maybe my room would look better if the color was blue.\n",
    "Assistant: {{\"action\": \"set_room_color\", \"action_input\": \"#0000FF\"}}\n",
    "User: Please set my room lighting to the color of a tree.\n",
    "Assistant: {{\"action\": \"set_room_color\", \"action_input\": \"#31D45B\"}}\n",
    "User: Thanks, Bye!\n",
    "Assistant: See you later.\n",
    "<</SYS>>\n",
    "\n",
    "{0}[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1279aff0-48d9-4587-8fdc-6c0800cb6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_room_color(hex):\n",
    "    # We would make a call to our smart-home API here, instead just drawing a colored square\n",
    "    display(HTML('<div style=\"width: 100px; height: 100px; background-color: ' + hex + '\"></div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec49b3a-74ca-4714-b591-43c76076add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_command(command):\n",
    "    # Put user command into prompt (in future projects we'll be re-injecting whole chat history here)\n",
    "    prompt = prompt_template.format(\"User: \" + command)\n",
    "    # Send command to the model\n",
    "    output = llm(prompt, stop=[\"User:\"])\n",
    "    response = output['choices'][0]['text']\n",
    "\n",
    "    # try to find json in the response\n",
    "    try:\n",
    "        # Extract json from model response by finding first and last brackets {}\n",
    "        firstBracketIndex = response.index(\"{\")\n",
    "        lastBracketIndex = len(response) - response[::-1].index(\"}\")\n",
    "        jsonString = response[firstBracketIndex:lastBracketIndex]\n",
    "        responseJson = json.loads(jsonString)\n",
    "        if responseJson['action'] == 'set_room_color':\n",
    "            set_room_color(responseJson['action_input'])\n",
    "            return 'Room color set to ' + responseJson['action_input'] + '.' \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # No json match, just return response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d21d64-7000-4e22-801f-fce6c831ec00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100px; height: 100px; background-color: #FFA07A\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Room color set to #FFA07A.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Can you make my room lighting orange please?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "075d87cf-bb59-4461-93bd-68cea780c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substring not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Hello! My name is Assistant. How can I assist you today? ðŸ˜Š'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a54b933-e408-4a6a-8115-f9bee860592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100px; height: 100px; background-color: #FFC900\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Room color set to #FFC900.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Please set the lights to a happy color.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6481c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525e4520-1968-43c8-9eab-a65afbe04f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananyahooda/opt/miniconda3/envs/aiml1/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1900: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'TheBloke/Llama-2-7B-Chat-GGML'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/Llama-2-7B-Chat-GGML' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Llama-2-7B-Chat-GGML\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m hf_auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_bXtcnKOaGDaXHUjskbRcpzQfZXauQbfsZl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_auth\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     16\u001b[0m     model_id, load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     17\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39mhf_auth\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/aiml1/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2013\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2008\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2010\u001b[0m     )\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2013\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2014\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2016\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2018\u001b[0m     )\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'TheBloke/Llama-2-7B-Chat-GGML'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/Llama-2-7B-Chat-GGML' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import transformers\n",
    "import subprocess\n",
    "#from llama_cpp import Llama\n",
    "\n",
    "model_id = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
    "\n",
    "hf_auth = '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'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    # stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "# ----------------------------------\n",
    "# Use LangChain to generate response\n",
    "# ----------------------------------\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "res = llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Experiment with LangChain Agent and Tool\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools import tool\n",
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "@tool\n",
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "@tool\n",
    "# Function to extract triplets from a sentence\n",
    "def triple_extracted(sentence):\n",
    "    # Generate the tokens using the triplet_extractor pipeline\n",
    "    tokens = triplet_extractor(sentence, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]\n",
    "    # Decode the tokens to get the text\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode(tokens)\n",
    "    # Extract the triplets from the text\n",
    "    extracted_triplets = extract_triplets(extracted_text[0])\n",
    "    return extracted_triplets\n",
    "\n",
    "@tool\n",
    "def create_directory(directory_name):\n",
    "    \"\"\"Function that creates a directory given a directory name.\"\"\"\"\"\n",
    "    subprocess.run([\"mkdir\", directory_name])\n",
    "    return json.dumps({\"directory_name\": directory_name})\n",
    "\n",
    "@tool\n",
    "def create_file(file_path):\n",
    "    \"\"\"Function that creates a file given a file path.\"\"\"\"\"\n",
    "    subprocess.run([\"touch\", file_path])\n",
    "    return json.dumps({\"file_path\": file_path})\n",
    "\n",
    "@tool\n",
    "def some_other_function():\n",
    "    \"\"\"Function that does something else.\"\"\"\"\"\n",
    "    return json.dumps({\"some\": \"response\"})    \n",
    "\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")\n",
    "tools = [create_directory, create_file, extract_triplets, triple_extracted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.output_parsers.json import parse_json_markdown\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "class OutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        try:\n",
    "            # this will work IF the text is a valid JSON with action and action_input\n",
    "            response = parse_json_markdown(text)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                # this means the agent is finished so we call AgentFinish\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                # otherwise the agent wants to use an action, so we call AgentAction\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            # sometimes the agent will return a string that is not a valid JSON\n",
    "            # often this happens when the agent is finished\n",
    "            # so we just return the text as the output\n",
    "            return AgentFinish({\"output\": text}, text)\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"conversational_chat\"\n",
    "\n",
    "\n",
    "# initialize output parser for agent\n",
    "parser = OutputParser()\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory,\n",
    "    agent_kwargs={\"output_parser\": parser}\n",
    ")\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
    "sys_msg = B_SYS + \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "All of Assistant's communication is performed using this JSON format.\n",
    "\n",
    "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n",
    "\n",
    "- \"Calculator\": Useful for when you need to answer questions about math.\n",
    "  - To use the calculator tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"action\": \"Calculator\",\n",
    "      \"action_input\": \"sqrt(4)\"}}\n",
    "    ```\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
    "```\n",
    "User: I'm great, what is the square root of 4?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Calculator\",\n",
    " \"action_input\": \"sqrt(4)\"}}\n",
    "```\n",
    "User: 2.0\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"It looks like the answer is 2!\"}}\n",
    "```\n",
    "User: Thanks could you tell me what 4 to the power of 2 is?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Calculator\",\n",
    " \"action_input\": \"4**2\"}}\n",
    "```\n",
    "User: 16.0\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"It looks like the answer is 16!\"}}\n",
    "```\n",
    "User: Thanks then could you tell me what is 16 multiplied by 3 is?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Calculator\",\n",
    " \"action_input\": \"16*3\"}}\n",
    "```\n",
    "User: 48.0\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"It looks like the answer is 48!\"}}\n",
    "```\n",
    "Here is the latest conversation between Assistant and User.\"\"\" + E_SYS\n",
    "\n",
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    ")\n",
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "\n",
    "instruction = B_INST + \" Respond to the following in JSON with 'action' and 'action_input' values \" + E_INST\n",
    "human_msg = instruction + \"\\nUser: {input}\"\n",
    "\n",
    "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg\n",
    "\n",
    "print(agent(\"hey how are you today?\"))\n",
    "\n",
    "print(agent(\"what is 4 to the power of 2.1?\"))\n",
    "\n",
    "print(agent(\"can you multiply that by 3?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
